{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a743fb1",
   "metadata": {},
   "source": [
    "## What is deep reinforcement learning\n",
    "\n",
    "It's a program that solve complex decision-making problems under uncertainty. These computer programs called *agents*. An agent is a decision maker only and nothing else. That means if you’re training a robot to pick up objects, the robot arm isn’t part of the agent. Only the code that makes decisions is referred to as the agent.\n",
    "\n",
    "The environment is everything outside the\n",
    "agent; everything the agent has no total control over.\n",
    "\n",
    "This strict boundary between the agent and the environment is counterintuitive at first,\n",
    "but the decision maker, the agent, can only have a single role: making decisions. Everything\n",
    "that comes after the decision gets bundled into the environment.\n",
    "\n",
    "The environment commonly has a well-defined task. The goal of this task is defined through\n",
    "the reward function. The reward-function signals can be simultaneously sequential, evalua-\n",
    "tive, and sampled. To achieve the goal, the agent needs to demonstrate intelligence, or at least\n",
    "cognitive abilities commonly associated with intelligence, such as long-term thinking, infor-\n",
    "mation gathering, and generalization.\n",
    "\n",
    "The agent has a three-step process: the agent interacts with the environment, the agent eval-\n",
    "uates its behavior, and the agent improves its responses. The agent can be designed to learn\n",
    "mappings from observations to actions called policies. The agent can be designed to learn the\n",
    "model of the environment on mappings called models. The agent can be designed to learn to\n",
    "estimate the reward-to-go on mappings called value functions.\n",
    "\n",
    "The interactions between the agent and the environment go on for several cycles. Each cycle\n",
    "is called a **time step**. At each time step, the agent observes the environment, takes action, and\n",
    "receives a new observation and reward. The set of the state, the action, the reward, and the\n",
    "new state is called an **experience**. Every experience has an opportunity for learning and\n",
    "improving performance.\n",
    "\n",
    "The task the agent is trying to solve may or may not have a natural ending. Tasks that have a\n",
    "natural ending, such as a game, are called **episodic tasks**. Conversely, tasks that don’t are called\n",
    "**continuing tasks**, such as learning forward motion. The sequence of time steps from the beginning to the end of an episodic task is called an episode. Agents may take several time steps and\n",
    "episodes to learn to solve a task. Agents learn through trial and error: they try something,\n",
    "observe, learn, try something else, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9eb6f",
   "metadata": {},
   "source": [
    "### The agent: The decision maker\n",
    "\n",
    "Agents have a three-step process: all agents\n",
    "have an interaction component, a way to gather data for learning; all agents evaluate their\n",
    "current behavior; and all agents improve something in their inner components that allows\n",
    "them to improve (or at least attempt to improve) their overall performance.\n",
    "\n",
    "### The environment: Everything else\n",
    "Most real-world decision-making problems can be expressed as RL environments. A com-\n",
    "mon way to represent decision-making processes in RL is by modeling the problem using a\n",
    "mathematical framework known as Markov decision processes (MDPs). In RL, we assume\n",
    "all environments have an MDP working under the hood. Whether an Atari game, the stock\n",
    "market, a self-driving car, your significant other, you name it, every problem has an MDP\n",
    "running under the hood (at least in the RL world, whether right or wrong).\n",
    "\n",
    "The environment is represented by a set of variables related to the problem. The combina-\n",
    "tion of all the possible values this set of variables can take is referred to as the state space. A\n",
    "state is a specific set of values the variables take at any given time.\n",
    "Agents may or may not have access to the actual environment’s state; however, one way or\n",
    "another, agents can observe something from the environment. The set of variables the agent\n",
    "perceives at any given time is called an observation.\n",
    "\n",
    "The combination of all possible values these variables can take is the observation space.\n",
    "Know that state and observation are terms used interchangeably in the RL community. This\n",
    "is because often agents are allowed to see the internal state of the environment, but this isn’t\n",
    "always the case. In this book, I use state and observation interchangeably as well. But you\n",
    "need to know that there might be a difference between states and observations, even though\n",
    "the RL community often uses the terms interchangeably.\n",
    "\n",
    "At every state, the environment makes available a set of actions the agent can choose from.\n",
    "Often the set of actions is the same for all states, but this isn’t required. The set of all actions\n",
    "in all states is referred to as the action space.\n",
    "\n",
    "The agent attempts to influence the environment through these actions. The environment\n",
    "may change states as a response to the agent’s action. The function that is responsible for this\n",
    "transition is called the transition function.\n",
    "\n",
    "After a transition, the environment emits a new observation. The environment may also\n",
    "provide a reward signal as a response. The function responsible for this mapping is called the\n",
    "reward function. The set of transition and reward function is referred to as the model of\n",
    "the environment.\n",
    "\n",
    "##### Bandit walk environment\n",
    "\n",
    "Bandit walk (BW) is a simple *grid-world* (GW) environment. GWs are a common type of environment for\n",
    "studying RL algorithms that are grids of any size. GWs can have any model (transition and\n",
    "reward functions) you can think of and can make any kind of actions available.\n",
    "\n",
    "But, they all commonly make move actions available to the agent: Left, Down, Right, Up\n",
    "(or West, South, East, North, which is more precise because the agent has no heading and\n",
    "usually has no visibility of the full grid, but cardinal directions can also be more confusing).\n",
    "And, of course, each action corresponds with its logical transition: Left goes left, and Right\n",
    "goes right. Also, they all tend to have a fully observable discrete state and observation spaces\n",
    "(that is, state equals observation) with integers representing the cell id location of the agent.\n",
    "A “walk” is a special case of grid-world environments with a single row. In reality, what I call a\n",
    "“walk” is more commonly referred to as a “Corridor.” But, in this book, I use the term “walk” for\n",
    "all the grid-world environments with a single row.\n",
    "\n",
    "The bandit walk (BW) is a walk with three states, but only one non-terminal state.\n",
    "Environments that have a single non-terminal state are called “bandit” environments. “Bandit”\n",
    "here is an analogy to slot machines, which are also known as “one-armed bandits”; they have\n",
    "one arm and, if you like gambling, can empty your pockets, the same way a bandit would.\n",
    "The BW environment has just two actions available: a Left (action 0) and an Right (action 1)\n",
    "action. BW has a deterministic transition function: a Left action always moves the agent to\n",
    "the Left, and a Right action always moves the agent to the right. The reward signal is a +1\n",
    "when landing on the rightmost cell, 0 otherwise. The agent starts in the middle cell.\n",
    "\n",
    "##### The bandit slippery walk environment\n",
    "\n",
    "Let’s say the surface of the walk is slippery and each action has a 20% chance of sending\n",
    "the agent backwards. I call this environment the bandit slippery walk (BSW)\n",
    "\n",
    "BSW is still a one-row-grid world, a walk, a corridor, with only Left and Right actions avail-\n",
    "able. Again, three states and two actions. The reward is the same as before, +1 when landing\n",
    "at the rightmost state (except when coming from the rightmost state-from itself ), and zero\n",
    "otherwise.\n",
    "\n",
    "However, the transition function is different: 80% of the time the agent moves to the\n",
    "intended cell, and 20% of time in the opposite direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a48b49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3caf2df7",
   "metadata": {},
   "source": [
    "###### States: Specific configurations of the environment\n",
    "\n",
    "A state is a unique and self-contained configuration of the problem. The set of all possible\n",
    "states, the state space, is defined as the set S. The state space can be finite or infinite. But notice\n",
    "that the state space is different than the set of variables that compose a single state. This other\n",
    "set must always be finite and of constant size from state to state. In the end, the state space is\n",
    "a set of sets. The inner set must be of equal size and finite, as it contains the number of variables representing the states, but the outer set can be infinite depending on the types of elements of the inner sets.\n",
    "\n",
    "<img src=\"./static/imgs/drl/math.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c4a35",
   "metadata": {},
   "source": [
    "##### Actions: A mechanism to influence the environment\n",
    "\n",
    "A is a function that takes a state as an argument;\n",
    "that is, A(s). This function returns the set of available actions for state s. If needed, you can\n",
    "define this set to be constant across the state space; that is, all actions are available at every\n",
    "state. You can also set all transitions from a state-action pair to zero if you want to deny an\n",
    "action in a given state. You could also set all transitions from state s and action a to the same\n",
    "state s to denote action a as a no-intervene or no-op action.\n",
    "\n",
    "Just as with the state, the action space may be finite or infinite, and the set of variables of a\n",
    "single action may contain more than one element and must be finite. However, unlike the\n",
    "number of state variables, the number of variables that compose an action may not be con-\n",
    "stant. The actions available in a state may change depending on that state. For simplicity,\n",
    "most environments are designed with the same number of actions in all states.\n",
    "\n",
    "##### Transition function: Consequences of agent actions\n",
    "\n",
    "The way the environment changes as a response to actions is referred to as the state-transition\n",
    "probabilities, or more simply, the transition function, and is denoted by $T(s, a, s')$. The transi-\n",
    "tion function T maps a transition tuple `s, a, s'` to a probability; that is, you pass in a state s, an\n",
    "action a, and a next state s', and it’ll return the corresponding probability of transition from\n",
    "state s to state s' when taking action a. You could also represent it as $T(s, a)$ and return a dic-\n",
    "tionary with the next states for its keys and probabilities for its values.\n",
    "Notice that T also describes a probability distribution $p( · | s, a)$ determining how the system\n",
    "will evolve in an interaction cycle from selecting action a in state s. When integrating over the\n",
    "next states s', as any probability distribution, the sum of these probabilities must equal one.\n",
    "\n",
    "##### Reward signal: Carrots and sticks\n",
    "\n",
    "The reward function R maps a transition tuple s, a, s' to a scalar. The reward function gives a\n",
    "numeric signal of goodness to transitions. When the signal is positive, we can think of the\n",
    "reward as an income or a reward. Most problems have at least one positive signal—winning\n",
    "a chess match or reaching the desired destination, for example. But, rewards can also be neg-\n",
    "ative, and we can see these as cost, punishment, or penalty. In robotics, adding a time step\n",
    "cost is a common practice because we usually want to reach a goal, but within a number of\n",
    "time steps. One thing to clarify is that whether positive or negative, the scalar coming out of\n",
    "the reward function is always referred to as the reward. RL folks are happy folks.\n",
    "It’s also important to highlight that while the reward function can be represented as\n",
    "R(s,a,s'), which is explicit, we could also use R(s,a), or even R(s), depending on our needs.\n",
    "Sometimes rewarding the agent based on state is what we need; sometimes it makes more\n",
    "sense to use the action and the state. However, the most explicit way to represent the reward\n",
    "function is to use a state, action, and next state triplet. With that, we can compute the mar-\n",
    "ginalization over next states in R(s,a,s') to obtain R(s,a), and the marginalization over actions\n",
    "in R(s,a) to get R(s). But, once we’re in R(s) we can’t recover R(s,a) or R(s,a,s'), and once\n",
    "we’re in R(s,a) we can’t recover R(s,a,s')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3d4c0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T10:51:47.134000+07:00",
     "start_time": "2022-02-28T03:51:33.464Z"
    }
   },
   "outputs": [],
   "source": [
    "using ReinforcementLearning\n",
    "using GridWorlds\n",
    "using StableRNGs\n",
    "using Flux\n",
    "using Flux.Losses\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e630ed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T10:52:00.688000+07:00",
     "start_time": "2022-02-28T03:51:50.000Z"
    }
   },
   "outputs": [],
   "source": [
    "function RL.Experiment(\n",
    "    ::Val{:JuliaRL},\n",
    "    ::Val{:BasicDQN},\n",
    "    ::Val{:SingleRoomUndirected},\n",
    "    ::Nothing;\n",
    "    seed=123,\n",
    ")\n",
    "    rng = StableRNG(seed)\n",
    "\n",
    "    env = GridWorlds.SingleRoomUndirectedModule.SingleRoomUndirected(rng=rng)\n",
    "    env = GridWorlds.RLBaseEnv(env)\n",
    "    env = RLEnvs.StateTransformedEnv(env; state_mapping=x -> vec(Float32.(x)))\n",
    "    env = RewardTransformedEnv(env; reward_mapping = x -> x - convert(typeof(x), 0.01))\n",
    "    env = MaxTimeoutEnv(env, 240)\n",
    "\n",
    "    ns, na = length(state(env)), length(action_space(env))\n",
    "    agent = Agent(\n",
    "        policy=QBasedPolicy(\n",
    "            learner=BasicDQNLearner(\n",
    "                approximator=NeuralNetworkApproximator(\n",
    "                    model=Chain(\n",
    "                        Dense(ns, 128, relu; init=glorot_uniform(rng)),\n",
    "                        Dense(128, 128, relu; init=glorot_uniform(rng)),\n",
    "                        Dense(128, na; init=glorot_uniform(rng)),\n",
    "                    ) |> gpu,\n",
    "                    optimizer=ADAM(),\n",
    "                ),\n",
    "                batch_size=32,\n",
    "                min_replay_history=100,\n",
    "                loss_func=huber_loss,\n",
    "                rng=rng,\n",
    "            ),\n",
    "            explorer=EpsilonGreedyExplorer(\n",
    "                kind=:exp,\n",
    "                ϵ_stable=0.01,\n",
    "                decay_steps=500,\n",
    "                rng=rng,\n",
    "            ),\n",
    "        ),\n",
    "        trajectory=CircularArraySARTTrajectory(\n",
    "            capacity=1000,\n",
    "            state=Vector{Float32} => (ns,),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    stop_condition = StopAfterStep(10_000, is_show_progress=!haskey(ENV, \"CI\"))\n",
    "    hook = TotalRewardPerEpisode()\n",
    "    Experiment(agent, env, stop_condition, hook, \"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd4bc28c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T10:54:03.583000+07:00",
     "start_time": "2022-02-28T03:52:03.814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [],
      "text/markdown": [],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:41\u001b[39mm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[1mTotal reward per episode\u001b[22m⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n",
      "            \u001b[90m┌────────────────────────────────────────┐\u001b[39m \n",
      "          \u001b[90m1\u001b[39m \u001b[90m│\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢠\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡰\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⢀\u001b[39m\u001b[38;5;2m⡆\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡄\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "            \u001b[90m│\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡆\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢠\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⣼\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡜\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "            \u001b[90m│\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣷\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⣼\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⢀\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⢰\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣦\u001b[39m\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "            \u001b[90m│\u001b[39m\u001b[0m⣀\u001b[38;5;2m⣇\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[0m⣀\u001b[0m⣀\u001b[38;5;2m⣏\u001b[39m\u001b[38;5;2m⣆\u001b[39m\u001b[38;5;2m⣏\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[0m⣀\u001b[38;5;2m⣸\u001b[39m\u001b[38;5;2m⣇\u001b[39m\u001b[38;5;2m⣸\u001b[39m\u001b[0m⣀\u001b[0m⣀\u001b[38;5;2m⣀\u001b[39m\u001b[0m⣀\u001b[38;5;2m⣄\u001b[39m\u001b[38;5;2m⣇\u001b[39m\u001b[0m⣀\u001b[38;5;2m⣇\u001b[39m\u001b[38;5;2m⣸\u001b[39m\u001b[0m⣀\u001b[38;5;2m⣀\u001b[39m\u001b[38;5;2m⣇\u001b[39m\u001b[38;5;2m⣇\u001b[39m\u001b[38;5;2m⣏\u001b[39m\u001b[38;5;2m⣇\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[0m⣀\u001b[0m⣀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⣀\u001b[0m⣀\u001b[0m⣀\u001b[38;5;2m⣇\u001b[39m\u001b[0m⣀\u001b[0m⣀\u001b[0m⣀\u001b[90m│\u001b[39m \n",
      "            \u001b[90m│\u001b[39m\u001b[38;5;2m⢰\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⢻\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⢹\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣸\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⣸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "            \u001b[90m│\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡄\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡟\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "            \u001b[90m│\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⣷\u001b[39m\u001b[38;5;2m⠁\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⣷\u001b[39m\u001b[38;5;2m⠁\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⣾\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡟\u001b[39m\u001b[38;5;2m⡄\u001b[39m\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⠁\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "   \u001b[0mScore    \u001b[90m│\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢰\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⠈\u001b[39m\u001b[38;5;2m⣾\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⠸\u001b[39m\u001b[38;5;2m⡀\u001b[39m\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "            \u001b[90m│\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢻\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "            \u001b[90m│\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡏\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡟\u001b[39m\u001b[38;5;2m⡄\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "            \u001b[90m│\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡟\u001b[39m\u001b[38;5;2m⣼\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⠁\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⣿\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "            \u001b[90m│\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⣿\u001b[39m\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⢻\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⡇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "            \u001b[90m│\u001b[39m\u001b[38;5;2m⠸\u001b[39m\u001b[38;5;2m⠸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⠸\u001b[39m\u001b[38;5;2m⠼\u001b[39m\u001b[38;5;2m⠸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⠧\u001b[39m\u001b[38;5;2m⠇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⠇\u001b[39m\u001b[38;5;2m⠇\u001b[39m\u001b[38;5;2m⠸\u001b[39m\u001b[38;5;2m⠇\u001b[39m\u001b[38;5;2m⠧\u001b[39m\u001b[38;5;2m⠤\u001b[39m\u001b[38;5;2m⠇\u001b[39m\u001b[38;5;2m⠿\u001b[39m\u001b[38;5;2m⠸\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[38;5;2m⠸\u001b[39m\u001b[38;5;2m⠇\u001b[39m\u001b[38;5;2m⠧\u001b[39m\u001b[38;5;2m⠇\u001b[39m\u001b[0m⠀\u001b[38;5;2m⠸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⠸\u001b[39m\u001b[0m⠀\u001b[38;5;2m⠧\u001b[39m\u001b[38;5;2m⠤\u001b[39m\u001b[38;5;2m⠇\u001b[39m\u001b[38;5;2m⠧\u001b[39m\u001b[38;5;2m⠤\u001b[39m\u001b[38;5;2m⠤\u001b[39m\u001b[38;5;2m⠇\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "            \u001b[90m│\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "         \u001b[90m-3\u001b[39m \u001b[90m│\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \n",
      "            \u001b[90m└────────────────────────────────────────┘\u001b[39m \n",
      "            ⠀\u001b[90m0\u001b[39m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[90m70\u001b[39m⠀ \n",
      "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[0mEpisode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip460\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip460)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip461\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip460)\" d=\"\n",
       "M147.478 1486.45 L2352.76 1486.45 L2352.76 47.2441 L147.478 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip462\">\n",
       "    <rect x=\"147\" y=\"47\" width=\"2206\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip462)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  176.868,1486.45 176.868,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip462)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  507.099,1486.45 507.099,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip462)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  837.329,1486.45 837.329,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip462)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1167.56,1486.45 1167.56,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip462)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1497.79,1486.45 1497.79,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip462)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1828.02,1486.45 1828.02,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip462)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2158.25,1486.45 2158.25,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  147.478,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  176.868,1486.45 176.868,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  507.099,1486.45 507.099,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  837.329,1486.45 837.329,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1167.56,1486.45 1167.56,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1497.79,1486.45 1497.79,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1828.02,1486.45 1828.02,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2158.25,1486.45 2158.25,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip460)\" d=\"M176.868 1517.37 Q173.257 1517.37 171.429 1520.93 Q169.623 1524.47 169.623 1531.6 Q169.623 1538.71 171.429 1542.27 Q173.257 1545.82 176.868 1545.82 Q180.503 1545.82 182.308 1542.27 Q184.137 1538.71 184.137 1531.6 Q184.137 1524.47 182.308 1520.93 Q180.503 1517.37 176.868 1517.37 M176.868 1513.66 Q182.679 1513.66 185.734 1518.27 Q188.813 1522.85 188.813 1531.6 Q188.813 1540.33 185.734 1544.94 Q182.679 1549.52 176.868 1549.52 Q171.058 1549.52 167.98 1544.94 Q164.924 1540.33 164.924 1531.6 Q164.924 1522.85 167.98 1518.27 Q171.058 1513.66 176.868 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M481.786 1544.91 L489.425 1544.91 L489.425 1518.55 L481.115 1520.21 L481.115 1515.95 L489.379 1514.29 L494.055 1514.29 L494.055 1544.91 L501.694 1544.91 L501.694 1548.85 L481.786 1548.85 L481.786 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M521.138 1517.37 Q517.527 1517.37 515.698 1520.93 Q513.893 1524.47 513.893 1531.6 Q513.893 1538.71 515.698 1542.27 Q517.527 1545.82 521.138 1545.82 Q524.772 1545.82 526.578 1542.27 Q528.406 1538.71 528.406 1531.6 Q528.406 1524.47 526.578 1520.93 Q524.772 1517.37 521.138 1517.37 M521.138 1513.66 Q526.948 1513.66 530.004 1518.27 Q533.082 1522.85 533.082 1531.6 Q533.082 1540.33 530.004 1544.94 Q526.948 1549.52 521.138 1549.52 Q515.328 1549.52 512.249 1544.94 Q509.194 1540.33 509.194 1531.6 Q509.194 1522.85 512.249 1518.27 Q515.328 1513.66 521.138 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M816.102 1544.91 L832.422 1544.91 L832.422 1548.85 L810.477 1548.85 L810.477 1544.91 Q813.139 1542.16 817.723 1537.53 Q822.329 1532.88 823.51 1531.53 Q825.755 1529.01 826.635 1527.27 Q827.537 1525.51 827.537 1523.82 Q827.537 1521.07 825.593 1519.33 Q823.672 1517.6 820.57 1517.6 Q818.371 1517.6 815.917 1518.36 Q813.487 1519.13 810.709 1520.68 L810.709 1515.95 Q813.533 1514.82 815.987 1514.24 Q818.44 1513.66 820.477 1513.66 Q825.848 1513.66 829.042 1516.35 Q832.236 1519.03 832.236 1523.52 Q832.236 1525.65 831.426 1527.57 Q830.639 1529.47 828.533 1532.07 Q827.954 1532.74 824.852 1535.95 Q821.75 1539.15 816.102 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M852.236 1517.37 Q848.625 1517.37 846.797 1520.93 Q844.991 1524.47 844.991 1531.6 Q844.991 1538.71 846.797 1542.27 Q848.625 1545.82 852.236 1545.82 Q855.871 1545.82 857.676 1542.27 Q859.505 1538.71 859.505 1531.6 Q859.505 1524.47 857.676 1520.93 Q855.871 1517.37 852.236 1517.37 M852.236 1513.66 Q858.047 1513.66 861.102 1518.27 Q864.181 1522.85 864.181 1531.6 Q864.181 1540.33 861.102 1544.94 Q858.047 1549.52 852.236 1549.52 Q846.426 1549.52 843.348 1544.94 Q840.292 1540.33 840.292 1531.6 Q840.292 1522.85 843.348 1518.27 Q846.426 1513.66 852.236 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M1156.4 1530.21 Q1159.76 1530.93 1161.63 1533.2 Q1163.53 1535.47 1163.53 1538.8 Q1163.53 1543.92 1160.01 1546.72 Q1156.49 1549.52 1150.01 1549.52 Q1147.84 1549.52 1145.52 1549.08 Q1143.23 1548.66 1140.78 1547.81 L1140.78 1543.29 Q1142.72 1544.43 1145.04 1545.01 Q1147.35 1545.58 1149.87 1545.58 Q1154.27 1545.58 1156.56 1543.85 Q1158.88 1542.11 1158.88 1538.8 Q1158.88 1535.75 1156.73 1534.03 Q1154.6 1532.3 1150.78 1532.3 L1146.75 1532.3 L1146.75 1528.45 L1150.96 1528.45 Q1154.41 1528.45 1156.24 1527.09 Q1158.07 1525.7 1158.07 1523.11 Q1158.07 1520.45 1156.17 1519.03 Q1154.3 1517.6 1150.78 1517.6 Q1148.86 1517.6 1146.66 1518.01 Q1144.46 1518.43 1141.82 1519.31 L1141.82 1515.14 Q1144.48 1514.4 1146.8 1514.03 Q1149.13 1513.66 1151.19 1513.66 Q1156.52 1513.66 1159.62 1516.09 Q1162.72 1518.5 1162.72 1522.62 Q1162.72 1525.49 1161.08 1527.48 Q1159.43 1529.45 1156.4 1530.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M1182.4 1517.37 Q1178.79 1517.37 1176.96 1520.93 Q1175.15 1524.47 1175.15 1531.6 Q1175.15 1538.71 1176.96 1542.27 Q1178.79 1545.82 1182.4 1545.82 Q1186.03 1545.82 1187.84 1542.27 Q1189.67 1538.71 1189.67 1531.6 Q1189.67 1524.47 1187.84 1520.93 Q1186.03 1517.37 1182.4 1517.37 M1182.4 1513.66 Q1188.21 1513.66 1191.26 1518.27 Q1194.34 1522.85 1194.34 1531.6 Q1194.34 1540.33 1191.26 1544.94 Q1188.21 1549.52 1182.4 1549.52 Q1176.59 1549.52 1173.51 1544.94 Q1170.45 1540.33 1170.45 1531.6 Q1170.45 1522.85 1173.51 1518.27 Q1176.59 1513.66 1182.4 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M1485.96 1518.36 L1474.16 1536.81 L1485.96 1536.81 L1485.96 1518.36 M1484.73 1514.29 L1490.61 1514.29 L1490.61 1536.81 L1495.54 1536.81 L1495.54 1540.7 L1490.61 1540.7 L1490.61 1548.85 L1485.96 1548.85 L1485.96 1540.7 L1470.36 1540.7 L1470.36 1536.19 L1484.73 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M1513.28 1517.37 Q1509.66 1517.37 1507.84 1520.93 Q1506.03 1524.47 1506.03 1531.6 Q1506.03 1538.71 1507.84 1542.27 Q1509.66 1545.82 1513.28 1545.82 Q1516.91 1545.82 1518.72 1542.27 Q1520.54 1538.71 1520.54 1531.6 Q1520.54 1524.47 1518.72 1520.93 Q1516.91 1517.37 1513.28 1517.37 M1513.28 1513.66 Q1519.09 1513.66 1522.14 1518.27 Q1525.22 1522.85 1525.22 1531.6 Q1525.22 1540.33 1522.14 1544.94 Q1519.09 1549.52 1513.28 1549.52 Q1507.47 1549.52 1504.39 1544.94 Q1501.33 1540.33 1501.33 1531.6 Q1501.33 1522.85 1504.39 1518.27 Q1507.47 1513.66 1513.28 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M1802.72 1514.29 L1821.08 1514.29 L1821.08 1518.22 L1807 1518.22 L1807 1526.7 Q1808.02 1526.35 1809.04 1526.19 Q1810.06 1526 1811.08 1526 Q1816.86 1526 1820.24 1529.17 Q1823.62 1532.34 1823.62 1537.76 Q1823.62 1543.34 1820.15 1546.44 Q1816.68 1549.52 1810.36 1549.52 Q1808.18 1549.52 1805.91 1549.15 Q1803.67 1548.78 1801.26 1548.04 L1801.26 1543.34 Q1803.34 1544.47 1805.57 1545.03 Q1807.79 1545.58 1810.27 1545.58 Q1814.27 1545.58 1816.61 1543.48 Q1818.95 1541.37 1818.95 1537.76 Q1818.95 1534.15 1816.61 1532.04 Q1814.27 1529.94 1810.27 1529.94 Q1808.39 1529.94 1806.52 1530.35 Q1804.66 1530.77 1802.72 1531.65 L1802.72 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M1842.83 1517.37 Q1839.22 1517.37 1837.39 1520.93 Q1835.59 1524.47 1835.59 1531.6 Q1835.59 1538.71 1837.39 1542.27 Q1839.22 1545.82 1842.83 1545.82 Q1846.47 1545.82 1848.27 1542.27 Q1850.1 1538.71 1850.1 1531.6 Q1850.1 1524.47 1848.27 1520.93 Q1846.47 1517.37 1842.83 1517.37 M1842.83 1513.66 Q1848.64 1513.66 1851.7 1518.27 Q1854.78 1522.85 1854.78 1531.6 Q1854.78 1540.33 1851.7 1544.94 Q1848.64 1549.52 1842.83 1549.52 Q1837.02 1549.52 1833.95 1544.94 Q1830.89 1540.33 1830.89 1531.6 Q1830.89 1522.85 1833.95 1518.27 Q1837.02 1513.66 1842.83 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M2143.66 1529.7 Q2140.51 1529.7 2138.66 1531.86 Q2136.83 1534.01 2136.83 1537.76 Q2136.83 1541.49 2138.66 1543.66 Q2140.51 1545.82 2143.66 1545.82 Q2146.8 1545.82 2148.63 1543.66 Q2150.48 1541.49 2150.48 1537.76 Q2150.48 1534.01 2148.63 1531.86 Q2146.8 1529.7 2143.66 1529.7 M2152.94 1515.05 L2152.94 1519.31 Q2151.18 1518.48 2149.37 1518.04 Q2147.59 1517.6 2145.83 1517.6 Q2141.2 1517.6 2138.75 1520.72 Q2136.32 1523.85 2135.97 1530.17 Q2137.34 1528.15 2139.4 1527.09 Q2141.46 1526 2143.93 1526 Q2149.14 1526 2152.15 1529.17 Q2155.18 1532.32 2155.18 1537.76 Q2155.18 1543.08 2152.04 1546.3 Q2148.89 1549.52 2143.66 1549.52 Q2137.66 1549.52 2134.49 1544.94 Q2131.32 1540.33 2131.32 1531.6 Q2131.32 1523.41 2135.21 1518.55 Q2139.1 1513.66 2145.65 1513.66 Q2147.41 1513.66 2149.19 1514.01 Q2150.99 1514.36 2152.94 1515.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M2173.24 1517.37 Q2169.63 1517.37 2167.8 1520.93 Q2165.99 1524.47 2165.99 1531.6 Q2165.99 1538.71 2167.8 1542.27 Q2169.63 1545.82 2173.24 1545.82 Q2176.87 1545.82 2178.68 1542.27 Q2180.51 1538.71 2180.51 1531.6 Q2180.51 1524.47 2178.68 1520.93 Q2176.87 1517.37 2173.24 1517.37 M2173.24 1513.66 Q2179.05 1513.66 2182.1 1518.27 Q2185.18 1522.85 2185.18 1531.6 Q2185.18 1540.33 2182.1 1544.94 Q2179.05 1549.52 2173.24 1549.52 Q2167.43 1549.52 2164.35 1544.94 Q2161.29 1540.33 2161.29 1531.6 Q2161.29 1522.85 2164.35 1518.27 Q2167.43 1513.66 2173.24 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip462)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  147.478,1285.51 2352.76,1285.51 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip462)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  147.478,884.997 2352.76,884.997 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip462)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  147.478,484.484 2352.76,484.484 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip462)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  147.478,83.9711 2352.76,83.9711 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  147.478,1486.45 147.478,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  147.478,1285.51 166.376,1285.51 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  147.478,884.997 166.376,884.997 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  147.478,484.484 166.376,484.484 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  147.478,83.9711 166.376,83.9711 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip460)\" d=\"M51.3625 1285.96 L81.0383 1285.96 L81.0383 1289.9 L51.3625 1289.9 L51.3625 1285.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M95.1586 1298.86 L111.478 1298.86 L111.478 1302.79 L89.5336 1302.79 L89.5336 1298.86 Q92.1956 1296.1 96.7789 1291.47 Q101.385 1286.82 102.566 1285.48 Q104.811 1282.95 105.691 1281.22 Q106.594 1279.46 106.594 1277.77 Q106.594 1275.01 104.649 1273.28 Q102.728 1271.54 99.6261 1271.54 Q97.4271 1271.54 94.9734 1272.3 Q92.5428 1273.07 89.7651 1274.62 L89.7651 1269.9 Q92.5891 1268.76 95.0428 1268.18 Q97.4965 1267.61 99.5335 1267.61 Q104.904 1267.61 108.098 1270.29 Q111.293 1272.98 111.293 1277.47 Q111.293 1279.6 110.483 1281.52 Q109.696 1283.42 107.589 1286.01 Q107.01 1286.68 103.909 1289.9 Q100.807 1293.09 95.1586 1298.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M50.9921 885.449 L80.6679 885.449 L80.6679 889.384 L50.9921 889.384 L50.9921 885.449 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M91.5706 898.342 L99.2095 898.342 L99.2095 871.977 L90.8993 873.643 L90.8993 869.384 L99.1632 867.717 L103.839 867.717 L103.839 898.342 L111.478 898.342 L111.478 902.277 L91.5706 902.277 L91.5706 898.342 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M99.5335 470.283 Q95.9224 470.283 94.0937 473.848 Q92.2882 477.389 92.2882 484.519 Q92.2882 491.625 94.0937 495.19 Q95.9224 498.732 99.5335 498.732 Q103.168 498.732 104.973 495.19 Q106.802 491.625 106.802 484.519 Q106.802 477.389 104.973 473.848 Q103.168 470.283 99.5335 470.283 M99.5335 466.579 Q105.344 466.579 108.399 471.186 Q111.478 475.769 111.478 484.519 Q111.478 493.246 108.399 497.852 Q105.344 502.436 99.5335 502.436 Q93.7234 502.436 90.6447 497.852 Q87.5892 493.246 87.5892 484.519 Q87.5892 475.769 90.6447 471.186 Q93.7234 466.579 99.5335 466.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M91.5706 97.316 L99.2095 97.316 L99.2095 70.9504 L90.8993 72.617 L90.8993 68.3578 L99.1632 66.6911 L103.839 66.6911 L103.839 97.316 L111.478 97.316 L111.478 101.251 L91.5706 101.251 L91.5706 97.316 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip462)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  209.891,1445.72 242.914,244.176 275.938,1445.72 308.961,87.9763 341.984,724.792 375.007,87.9763 408.03,1445.72 441.053,1445.72 474.076,696.756 507.099,1445.72 \n",
       "  540.122,95.9865 573.145,87.9763 606.168,1445.72 639.191,1445.72 672.214,148.053 705.237,516.525 738.26,252.187 771.283,1445.72 804.306,516.525 837.329,1445.72 \n",
       "  870.352,103.997 903.375,1445.72 936.398,1445.72 969.421,95.9865 1002.44,1445.72 1035.47,1445.72 1068.49,1445.72 1101.51,1445.72 1134.54,500.505 1167.56,1445.72 \n",
       "  1200.58,1445.72 1233.61,468.464 1266.63,1445.72 1299.65,424.407 1332.67,396.371 1365.7,128.028 1398.72,108.002 1431.74,1445.72 1464.77,1445.72 1497.79,348.31 \n",
       "  1530.81,1445.72 1563.84,1445.72 1596.86,496.5 1629.88,732.802 1662.9,180.094 1695.93,128.028 1728.95,1445.72 1761.97,340.3 1795,428.412 1828.02,1445.72 \n",
       "  1861.04,164.074 1894.07,380.351 1927.09,1445.72 1960.11,1445.72 1993.14,1445.72 2026.16,1445.72 2059.18,99.9917 2092.2,1445.72 2125.23,1445.72 2158.25,1445.72 \n",
       "  2191.27,1445.72 2224.3,1445.72 2257.32,1445.72 2290.34,91.9814 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip460)\" d=\"\n",
       "M1986.82 198.898 L2279.25 198.898 L2279.25 95.2176 L1986.82 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1986.82,198.898 2279.25,198.898 2279.25,95.2176 1986.82,95.2176 1986.82,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip460)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2010.82,147.058 2154.82,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip460)\" d=\"M2192.66 166.745 Q2190.85 171.375 2189.14 172.787 Q2187.43 174.199 2184.56 174.199 L2181.15 174.199 L2181.15 170.634 L2183.65 170.634 Q2185.41 170.634 2186.39 169.8 Q2187.36 168.967 2188.54 165.865 L2189.3 163.921 L2178.82 138.412 L2183.33 138.412 L2191.43 158.689 L2199.53 138.412 L2204.05 138.412 L2192.66 166.745 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip460)\" d=\"M2211.34 160.402 L2218.98 160.402 L2218.98 134.037 L2210.67 135.703 L2210.67 131.444 L2218.93 129.778 L2223.61 129.778 L2223.61 160.402 L2231.25 160.402 L2231.25 164.338 L2211.34 164.338 L2211.34 160.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = E`JuliaRL_BasicDQN_SingleRoomUndirected`\n",
    "run(ex)\n",
    "plot(ex.hook.rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a238e25",
   "metadata": {},
   "source": [
    "## Balancing immediate and long-term goals\n",
    "\n",
    "In this chapter, you’ll learn about algorithms for solving MDPs. We first discuss the\n",
    "objective of an agent and why simple plans are not sufficient to solve MDPs. We then talk\n",
    "about the two fundamental algorithms for solving MDPs under a technique called dynamic programming: value iteration (VI) and policy iteration (PI).\n",
    "\n",
    "\n",
    "You’ll soon notice that these methods in a way “cheat”: they require full access to the\n",
    "MDP, and they depend on knowing the dynamics of the environment, which is something we\n",
    "can’t always obtain. However, the fundamentals you’ll learn are still useful for learning about\n",
    "more advanced algorithms. In the end, VI and PI are the foundations from which virtually\n",
    "every other RL (and DRL) algorithm originates.\n",
    "\n",
    "### The objective of a decision-making agent\n",
    "\n",
    "At first, it seems the agent’s goal is to find a sequence of actions that will maximize the return:\n",
    "the sum of rewards (discounted or undiscounted—depending on the value of gamma) during\n",
    "an episode or the entire life of the agent, depending on the task.\n",
    "\n",
    "But this isn’t enough! The problem with plans is they don’t account for stochasticity in envi-\n",
    "ronments, and both the SWF and FL are stochastic; actions taken won’t always work the way\n",
    "we intend. What would happen if, due to the environment’s stochasticity, our agent lands on\n",
    "a cell not covered by our plan?\n",
    "\n",
    "What the agent needs to come up with is called a *policy*. Policies are universal plans; policies\n",
    "cover all possible states. We need to plan for every possible state. Policies can be stochastic or\n",
    "deterministic: the policy can return action-probability distributions or single actions for a\n",
    "given state (or observation). For now, we’re working with deterministic policies, which is\n",
    "a lookup table that maps actions to states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645d750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42d83d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c2d3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5bff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f0d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082fb1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b57dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa336e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb90cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8727e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a1118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d838b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3451109f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c25faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e6238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7713d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e8fa03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c4ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198aadb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
