{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004d328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using MLDatasets\n",
    "using ImageCore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d509f8",
   "metadata": {},
   "source": [
    "# Grokking Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0479ada7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022499999999999975"
     ]
    }
   ],
   "source": [
    "weight = 0.1\n",
    "lr = 0.01\n",
    "\n",
    "function neural_network(input, weight)\n",
    "    prediction = input * weight\n",
    "end\n",
    "\n",
    "number_of_toes = [8.5]\n",
    "win_or_lose_binary = [1]  # Won!\n",
    "\n",
    "input = number_of_toes[1]\n",
    "t = win_or_lose_binary[1]\n",
    "\n",
    "pred = neural_network(input, weight)\n",
    "error = (pred - t) ^ 2\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c293725",
   "metadata": {},
   "source": [
    "### Hot and cold learning (Simple but Inefficient)\n",
    "\n",
    "\n",
    "Hot and cold learning means wiggling the weights to see which direction reduces the error\n",
    "the most, moving the weights in that direction, and repeating until the error gets to 0.\n",
    "\n",
    "Well, you try both up and down and see which one reduces the error!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b44232",
   "metadata": {},
   "source": [
    "### Gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bac8eba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11275"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = 0.1\n",
    "\n",
    "function neural_network(input, weight)\n",
    "    prediction = input * weight\n",
    "end\n",
    "\n",
    "number_of_toes = [8.5]\n",
    "win_or_lose_binary = [1]  # Won!\n",
    "\n",
    "input = number_of_toes[1]\n",
    "goal_pred = win_or_lose_binary[1]\n",
    "\n",
    "pred = neural_network(input, weight)\n",
    "error = (pred - goal_pred) ^ 2\n",
    "∇ = pred - goal_pred\n",
    "weight∇ = input * ∇\n",
    "α = 0.01\n",
    "weight -= weight∇ * α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "913de45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1\n",
      "ϵ = 1.0\n",
      "weight = 0.0\n",
      "pred = 0.0\n",
      "=== Epoch 2\n",
      "ϵ = 0.03609999999999998\n",
      "weight = 0.9\n",
      "pred = 0.81\n",
      "=== Epoch 3\n",
      "ϵ = 0.0013032100000000015\n",
      "weight = 1.071\n",
      "pred = 0.9639\n",
      "=== Epoch 4\n",
      "ϵ = 4.704588100000082e-5\n",
      "weight = 1.1034899999999999\n",
      "pred = 0.9931409999999999\n",
      "=== Epoch 5\n",
      "ϵ = 1.698356304100287e-6\n",
      "weight = 1.1096631\n",
      "pred = 0.9986967899999999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.110835989"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight, goad_pred, input = (0.0, 0.8, 0.9)\n",
    "\n",
    "for i in 1:5\n",
    "    pred = input * weight\n",
    "    ϵ = (pred - goal_pred) ^ 2\n",
    "    println(\"=== Epoch $(i)\")\n",
    "    @show ϵ weight pred\n",
    "    ∇ = pred - goal_pred\n",
    "    weight∇ = input * ∇\n",
    "    weight -= weight∇\n",
    "end\n",
    "\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1ccd9860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1\n",
      "ϵ = 1.0\n",
      "weight = 0.0\n",
      "pred = 0.0\n",
      "∇ = -1.0\n",
      "=== Epoch 2\n",
      "ϵ = 0.354025\n",
      "weight = 0.45\n",
      "pred = 0.405\n",
      "∇ = -0.595\n",
      "=== Epoch 3\n",
      "ϵ = 0.12533370062500002\n",
      "weight = 0.71775\n",
      "pred = 0.645975\n",
      "∇ = -0.35402500000000003\n",
      "=== Epoch 4\n",
      "ϵ = 0.0443712633637656\n",
      "weight = 0.87706125\n",
      "pred = 0.789355125\n",
      "∇ = -0.21064487499999995\n",
      "=== Epoch 5\n",
      "ϵ = 0.015708536512357138\n",
      "weight = 0.97185144375\n",
      "pred = 0.874666299375\n",
      "∇ = -0.12533370062500004\n",
      "=== Epoch 6\n",
      "ϵ = 0.0055612146387872445\n",
      "weight = 1.02825160903125\n",
      "pred = 0.9254264481281249\n",
      "∇ = -0.07457355187187509\n",
      "=== Epoch 7\n",
      "ϵ = 0.0019688090124966493\n",
      "weight = 1.0618097073735937\n",
      "pred = 0.9556287366362344\n",
      "∇ = -0.04437126336376562\n",
      "=== Epoch 8\n",
      "ϵ = 0.0006970076106491235\n",
      "weight = 1.0817767758872883\n",
      "pred = 0.9735990982985595\n",
      "∇ = -0.026400901701440493\n",
      "=== Epoch 9\n",
      "ϵ = 0.0002467581193600543\n",
      "weight = 1.0936571816529366\n",
      "pred = 0.984291463487643\n",
      "∇ = -0.01570853651235704\n",
      "=== Epoch 10\n",
      "ϵ = 8.73585432064442e-5\n",
      "weight = 1.1007260230834972\n",
      "pred = 0.9906534207751475\n",
      "∇ = -0.009346579224852491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.104931983734681"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight, goad_pred, input = (0.0, 0.8, 0.9)\n",
    "α = 0.5\n",
    "\n",
    "for i in 1:10\n",
    "    pred = input * weight\n",
    "    ϵ = (pred - goal_pred) ^ 2\n",
    "    println(\"=== Epoch $(i)\")\n",
    "    ∇ = pred - goal_pred\n",
    "    @show ϵ weight pred ∇\n",
    "    weight∇ = input * ∇\n",
    "    weight -= (α * weight∇)\n",
    "end\n",
    "\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "77923814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wsum (generic function with 1 method)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsum(a, b) = sum(a .* b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "024c4dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neural_network (generic function with 1 method)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function neural_network(input, weights)\n",
    "    wsum(input, weights)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "be1e7584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1\n",
      "ϵ = 0.00122500000000001\n",
      "α = 0.003\n",
      "∇ = 0.03500000000000014\n",
      "weights = [0.09745000000000001, 0.099805, 0.09964]\n",
      "pred = 1.0350000000000001\n",
      "=== Epoch 2\n",
      "ϵ = 0.00016297713906250229\n",
      "α = 0.003\n",
      "∇ = 0.01276625000000009\n",
      "weights = [0.09496502500000001, 0.09961038025, 0.099281296]\n",
      "pred = 1.01276625\n",
      "=== Epoch 3\n",
      "ϵ = 7.944130406129285e-5\n",
      "α = 0.003\n",
      "∇ = -0.00891298513749983\n",
      "weights = [0.0925434168625, 0.09941614000851251, 0.09892388333440001]\n",
      "pred = 0.9910870148625002\n",
      "=== Epoch 4\n",
      "ϵ = 0.0009031110235428149\n",
      "α = 0.003\n",
      "∇ = -0.03005180566193677\n",
      "weights = [0.09018355973250626, 0.09922227853549591, 0.09856775735439617]\n",
      "pred = 0.9699481943380632\n"
     ]
    }
   ],
   "source": [
    "toes = [8.5 , 9.5, 9.9, 9.0]\n",
    "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
    "nfans = [1.2 , 1.3, 0.5, 1.0]\n",
    "\n",
    "win_or_lose_binary = [1, 1, 0, 1]\n",
    "\n",
    "weights = [0.1, 0.1, 0.1]\n",
    "input = [toes[1], wlrec[1], nfans[1]]\n",
    "\n",
    "for i in 1:4\n",
    "    pred = neural_network(input, weights)\n",
    "    ∇ = pred - win_or_lose_binary[1]\n",
    "    ϵ = ∇ ^ 2\n",
    "    α = 0.003\n",
    "    weight_∇ = input .* weights\n",
    "    weights = weights .- (weight_∇ .* α)\n",
    "    println(\"=== Epoch $(i)\")\n",
    "    @show ϵ α ∇ weights pred\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b06c9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full training set\n",
    "train_x, train_y = MNIST.traindata();\n",
    "\n",
    "# load full test set\n",
    "test_x,  test_y  = MNIST.testdata();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b77c8d",
   "metadata": {},
   "source": [
    "## Building a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a1a64008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1\n",
      "total_error = 2.6561231104\n",
      "=== Epoch 2\n",
      "total_error = 0.9628701776715985\n",
      "=== Epoch 3\n",
      "total_error = 0.5509165866836797\n",
      "=== Epoch 4\n",
      "total_error = 0.36445836852222424\n",
      "=== Epoch 5\n",
      "total_error = 0.2516768662079895\n",
      "=== Epoch 6\n",
      "total_error = 0.17797575048089034\n",
      "=== Epoch 7\n",
      "total_error = 0.12864460733422164\n",
      "=== Epoch 8\n",
      "total_error = 0.09511036950476208\n",
      "=== Epoch 9\n",
      "total_error = 0.07194564247043436\n",
      "=== Epoch 10\n",
      "total_error = 0.05564914990717743\n",
      "=== Epoch 11\n",
      "total_error = 0.04394763937673939\n",
      "=== Epoch 12\n",
      "total_error = 0.035357967050948465\n",
      "=== Epoch 13\n",
      "total_error = 0.02890700056547436\n",
      "=== Epoch 14\n",
      "total_error = 0.023951660591138853\n",
      "=== Epoch 15\n",
      "total_error = 0.020063105176016144\n",
      "=== Epoch 16\n",
      "total_error = 0.016952094519447087\n",
      "=== Epoch 17\n",
      "total_error = 0.014420818295271236\n",
      "=== Epoch 18\n",
      "total_error = 0.012331739998443648\n",
      "=== Epoch 19\n",
      "total_error = 0.010587393171639842\n",
      "=== Epoch 20\n",
      "total_error = 0.009117233405426495\n"
     ]
    }
   ],
   "source": [
    "weights = [0.5, 0.48, -0.7]\n",
    "α = 0.1\n",
    "\n",
    "streetlights = [1 0 1; 0 1 1; 0 0 1; 1 1 1; 0 1 1; 1 0 1]\n",
    "walk_vs_stop = [0 1 0 1 1 0]\n",
    "\n",
    "for i in 1:20\n",
    "    println(\"=== Epoch $(i)\")\n",
    "    total_error = 0\n",
    "    for (j, row) in enumerate(eachrow(streetlights))\n",
    "        goal_pred = walk_vs_stop[j]\n",
    "        pred = wsum(row, weights)\n",
    "        error = (goal_pred - pred) ^ 2\n",
    "        total_error += error\n",
    "        ∇ = pred - goal_pred\n",
    "        weights = weights .- (α .* (row .* ∇ ))\n",
    "    end\n",
    "    @show total_error\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0c768",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "8a53a5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1\n",
      "total_error = 5.402956997437806\n",
      "=== Epoch 2\n",
      "total_error = 1.9251309373400844\n",
      "=== Epoch 3\n",
      "total_error = 0.8428460921954337\n",
      "=== Epoch 4\n",
      "total_error = 0.3566410966847956\n",
      "=== Epoch 5\n",
      "total_error = 0.1603395974073259\n",
      "=== Epoch 6\n",
      "total_error = 0.09178367336855531\n",
      "=== Epoch 7\n",
      "total_error = 0.05437860206554075\n",
      "=== Epoch 8\n",
      "total_error = 0.034496313133492953\n",
      "=== Epoch 9\n",
      "total_error = 0.020195104522025862\n",
      "=== Epoch 10\n",
      "total_error = 0.011630918423517662\n",
      "=== Epoch 11\n",
      "total_error = 0.006646104407267865\n",
      "=== Epoch 12\n",
      "total_error = 0.003799964968302484\n",
      "=== Epoch 13\n",
      "total_error = 0.0021920647615691793\n",
      "=== Epoch 14\n",
      "total_error = 0.001286041394114329\n",
      "=== Epoch 15\n",
      "total_error = 0.0007730115285601523\n",
      "=== Epoch 16\n",
      "total_error = 0.0004790285056091288\n",
      "=== Epoch 17\n",
      "total_error = 0.0003074174328568179\n",
      "=== Epoch 18\n",
      "total_error = 0.00020475183342557235\n",
      "=== Epoch 19\n",
      "total_error = 0.00014148576942701882\n",
      "=== Epoch 20\n",
      "total_error = 0.00010117794922473382\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 4\n",
    "weights₀ = 2 * rand(Float64, (3, hidden_size)) .- 1\n",
    "weights₁ = 2 * rand(Float64, (hidden_size, 1)) .- 1\n",
    "α = 0.2\n",
    "\n",
    "relu(x) = (x > 0) * x\n",
    "\n",
    "relu2deriv(output) = output > 0\n",
    "\n",
    "streetlights = [1 0 1; 0 1 1; 0 0 1; 1 1 1; 0 1 1; 1 0 1]\n",
    "walk_vs_stop = [0 1 0 1 1 0]\n",
    "\n",
    "for epoch in 1:20\n",
    "    println(\"=== Epoch $(epoch)\")\n",
    "    total_error = 0\n",
    "    for (i, row) in enumerate(eachrow(streetlights))\n",
    "        layer₀ = row'\n",
    "        goal_pred = walk_vs_stop[i]\n",
    "        layer₁ = relu.(layer₀ * weights₀)\n",
    "        \n",
    "        layer₂ = sum(layer₁ * weights₁)\n",
    "        \n",
    "        total_error += (layer₂ - goal_pred) ^ 2\n",
    "        layer₂∇ = layer₂ - goal_pred\n",
    "        \n",
    "        layer₁∇ = (layer₂∇ * weights₁') .* relu2deriv.(layer₁)\n",
    "        weights₁ -= α * (layer₁' * layer₂∇)\n",
    "        weights₀ -= α * (layer₀' * layer₁∇)\n",
    "    end\n",
    "    @show total_error\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b02d57",
   "metadata": {},
   "source": [
    "### Three-layer network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bc9a0a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 50\n",
      "error = 0.43363680539958976\n",
      "correct = 0.7665\n",
      "test_error = 0.33530167442822706\n",
      "test_correct = 0.8524\n",
      "=== Epoch 100\n",
      "error = 0.38558283436807195\n",
      "correct = 0.8065\n",
      "test_error = 0.3015177073133007\n",
      "test_correct = 0.8742\n",
      "=== Epoch 150\n",
      "error = 0.34570466776592645\n",
      "correct = 0.838\n",
      "test_error = 0.29056989247280834\n",
      "test_correct = 0.88\n",
      "=== Epoch 200\n",
      "error = 0.3309050307048\n",
      "correct = 0.858\n",
      "test_error = 0.28204327293173254\n",
      "test_correct = 0.8847\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using MLDatasets\n",
    "using ImageCore\n",
    "using Distributions\n",
    "\n",
    "p = Binomial(1, 0.5)\n",
    "\n",
    "one_hot_encoding(x) = Flux.onehotbatch(x, 0:9)\n",
    "train_size = 2000\n",
    "\n",
    "train_x, train_y = MNIST.traindata(1:train_size);\n",
    "test_x, test_y  = MNIST.testdata();\n",
    "\n",
    "labels = one_hot_encoding(train_y)\n",
    "test_labels = one_hot_encoding(test_y)\n",
    "\n",
    "α, iterations =  0.001, 201\n",
    "hidden_size, pixels_per_image, num_labels = 40, 28 * 28, 10\n",
    "batch_size = 100\n",
    "\n",
    "weights₀ = 0.2 * rand(Float16, (pixels_per_image, hidden_size)) .- 0.1\n",
    "weights₁ = 0.2 * rand(Float16, (hidden_size, num_labels)) .- 0.1\n",
    "\n",
    "relu(x) = (x > 0) * x\n",
    "\n",
    "relu2deriv(output) = output > 0\n",
    "\n",
    "for epoch in 1:iterations\n",
    "    error, correct = 0, 0\n",
    "    for i in 1:train_size\n",
    "        layer₀ = reshape(train_x[:, :, i], 1, 28*28)  # Input\n",
    "        label = reshape(labels[:, i], 1, 10)\n",
    "\n",
    "        layer₁ = relu.(layer₀ * weights₀)\n",
    "        dropout_mask = rand(p, size(layer₁))\n",
    "        layer₁ = (layer₁ .* dropout_mask) * 2\n",
    "\n",
    "        layer₂ = layer₁ * weights₁\n",
    "        error += sum(( label - layer₂ ) .^ 2)\n",
    "        correct += convert(Int, argmax(layer₂) == argmax(label))\n",
    "\n",
    "        layer₂∇ = label - layer₂\n",
    "        layer₁∇ = (layer₂∇ * weights₁') .* relu2deriv.(layer₁)\n",
    "        layer₁∇ = layer₁∇ .* dropout_mask\n",
    "\n",
    "        weights₁ += α * (layer₁' * layer₂∇)\n",
    "        weights₀ += α * (layer₀' * layer₁∇)\n",
    "    end\n",
    "\n",
    "    if rem(epoch, 50) == 0\n",
    "        println(\"=== Epoch $(epoch)\")\n",
    "        test_size = 10000\n",
    "        correct /= train_size\n",
    "        error /= train_size\n",
    "        @show error correct\n",
    "        test_error, test_correct = (0.0, 0)\n",
    "        for i in 1:test_size\n",
    "            layer₀ = reshape(test_x[:, :, i], 1, 28*28)\n",
    "            label = reshape(test_labels[:, i], 1, 10)\n",
    "            layer₁ = relu.(layer₀ * weights₀)\n",
    "            layer₂ = layer₁ * weights₁\n",
    "            test_error += sum(( label - layer₂ ) .^ 2)\n",
    "            test_correct += convert(Int, argmax(layer₂) == argmax(label))\n",
    "        end\n",
    "        test_error /= test_size\n",
    "        test_correct /= test_size\n",
    "        @show test_error test_correct\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "32f09378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict = Any[3, 1, 5, 3, 1, 8, 4, 9, 7, 7, 1, 6, 6, 0, 8, 2]\n",
      "label = Any[3, 1, 3, 3, 1, 8, 4, 9, 7, 7, 7, 6, 6, 0, 8, 2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "28×448 MosaicView{Gray{N0f8}, 4, Base.ReshapedArray{Gray{N0f8}, 4, PaddedView{Gray{N0f8}, 3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, StackViews.StackView{Gray{N0f8}, 3, 3, Vector{OffsetArrays.OffsetMatrix{Gray{N0f8}, PaddedView{Gray{N0f8}, 2, Tuple{UnitRange{Int64}, UnitRange{Int64}}, Base.ReinterpretArray{Gray{N0f8}, 2, N0f8, Matrix{N0f8}, true}}}}}}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}}:\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                   \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Images\n",
    "using Flux: onecold\n",
    "\n",
    "function test_model(image)\n",
    "    layer₀ = reshape(image, 1, 28*28)\n",
    "    layer₁ = relu.(layer₀ * weights₀)\n",
    "    layer₂ = layer₁ * weights₁\n",
    "    argmax(layer₂)[2] - 1\n",
    "end\n",
    "\n",
    "n = rand(1:10000, 16)\n",
    "grid = [\n",
    "    MNIST.convert2image(test_x[:, :, i])\n",
    "    for i in n\n",
    "]\n",
    "predict, label = [], []\n",
    "\n",
    "for i in n\n",
    "    push!(predict, test_model(test_x[:, :, i]))\n",
    "    push!(label, onecold(test_labels[:, i]) - 1)\n",
    "end\n",
    "\n",
    "@show predict label\n",
    "\n",
    "mosaicview(grid; nrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe9d65",
   "metadata": {},
   "source": [
    "### Batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "16e0fb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 50\n",
      "error = 0.37425603499667937\n",
      "correct = 0.853\n",
      "test_error = 0.4191010159136493\n",
      "test_correct = 0.7974\n",
      "=== Epoch 100\n",
      "error = 0.2899693723800722\n",
      "correct = 0.912\n",
      "test_error = 0.3593513258346201\n",
      "test_correct = 0.8316\n",
      "=== Epoch 150\n",
      "error = 0.25272370040260533\n",
      "correct = 0.93\n",
      "test_error = 0.33472305231706106\n",
      "test_correct = 0.8412\n",
      "=== Epoch 200\n",
      "error = 0.214534084315744\n",
      "correct = 0.959\n",
      "test_error = 0.322661942996808\n",
      "test_correct = 0.8472\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using MLDatasets\n",
    "using ImageCore\n",
    "using Distributions\n",
    "\n",
    "one_hot_encoding(x) = Flux.onehotbatch(x, 0:9)\n",
    "train_size = 1000\n",
    "\n",
    "train_x, train_y = MNIST.traindata(1:train_size);\n",
    "test_x, test_y  = MNIST.testdata();\n",
    "\n",
    "labels = one_hot_encoding(train_y)\n",
    "test_labels = one_hot_encoding(test_y)\n",
    "\n",
    "α, iterations = 0.1, 201\n",
    "hidden_size, pixels_per_image, num_labels = 128, 28 * 28, 10\n",
    "batch_size = 100\n",
    "\n",
    "weights₀ = 0.2 * rand(Float16, (pixels_per_image, hidden_size)) .- 0.1\n",
    "weights₁ = 0.2 * rand(Float16, (hidden_size, num_labels)) .- 0.1\n",
    "\n",
    "relu(x) = (x > 0) * x\n",
    "\n",
    "relu2deriv(output) = output > 0\n",
    "\n",
    "for epoch in 1:iterations\n",
    "    error, correct = 0, 0\n",
    "    for i in 1:batch_size:train_size\n",
    "        layer₀ = reshape(train_x[:, :, i:i + batch_size - 1], (28*28, batch_size))'\n",
    "        label = labels[:, i:i + batch_size - 1]'\n",
    "\n",
    "        layer₁ = relu.(layer₀ * weights₀)\n",
    "        p = Binomial(1, 0.5)\n",
    "        dropout_mask = rand(p, size(layer₁))\n",
    "        layer₁ = (layer₁ .* dropout_mask) .* 2\n",
    "        layer₂ = layer₁ * weights₁\n",
    "\n",
    "        # @show size(layer₀) size(layer₁) size(layer₂) size(weights₀) size(weights₁)\n",
    "        error += sum(( label - layer₂ ) .^ 2)\n",
    "        correct += sum(convert.(Int, argmax(layer₂, dims=2) .== argmax(label, dims=2)))\n",
    "        layer₂∇ = (label - layer₂) ./ batch_size\n",
    "        layer₁∇ = (layer₂∇ * weights₁') .* relu2deriv.(layer₁)\n",
    "        layer₁∇ = layer₁∇ .* dropout_mask\n",
    "        weights₁ += α * (layer₁' * layer₂∇)\n",
    "        weights₀ += α * (layer₀' * layer₁∇)\n",
    "    end\n",
    "\n",
    "    if rem(epoch, 50) == 0\n",
    "        println(\"=== Epoch $(epoch)\")\n",
    "        test_size = 5000\n",
    "        correct /= train_size\n",
    "        error /= train_size\n",
    "        @show error correct\n",
    "        test_error, test_correct = (0.0, 0)\n",
    "        for i in 1:test_size\n",
    "            layer₀ = reshape(test_x[:, :, i], 1, 28*28)\n",
    "            label = test_labels[:, i]'\n",
    "            layer₁ = relu.(layer₀ * weights₀)\n",
    "            layer₂ = layer₁ * weights₁\n",
    "            test_error += sum(( label - layer₂ ) .^ 2)\n",
    "            test_correct += convert(Int, argmax(layer₂) == argmax(label))\n",
    "        end\n",
    "        test_error /= test_size\n",
    "        test_correct /= test_size\n",
    "        @show test_error test_correct\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2518b53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict = Any[4, 9, 2, 1, 6, 1, 0, 7, 3, 3, 7, 4, 2, 3, 0, 5]\n",
      "label = Any[4, 9, 2, 1, 6, 1, 0, 9, 3, 3, 7, 9, 2, 3, 0, 5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "28×448 MosaicView{Gray{N0f8}, 4, Base.ReshapedArray{Gray{N0f8}, 4, PaddedView{Gray{N0f8}, 3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, StackViews.StackView{Gray{N0f8}, 3, 3, Vector{OffsetArrays.OffsetMatrix{Gray{N0f8}, PaddedView{Gray{N0f8}, 2, Tuple{UnitRange{Int64}, UnitRange{Int64}}, Base.ReinterpretArray{Gray{N0f8}, 2, N0f8, Matrix{N0f8}, true}}}}}}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}}:\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                   \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test_model(image)\n",
    "    layer₀ = reshape(image, 1, 28*28)\n",
    "    layer₁ = relu.(layer₀ * weights₀)\n",
    "    layer₂ = layer₁ * weights₁\n",
    "    argmax(layer₂)[2] - 1\n",
    "end\n",
    "\n",
    "n = rand(1:10000, 16)\n",
    "grid = [\n",
    "    MNIST.convert2image(test_x[:, :, i])\n",
    "    for i in n\n",
    "]\n",
    "predict, label = [], []\n",
    "\n",
    "for i in n\n",
    "    push!(predict, test_model(test_x[:, :, i]))\n",
    "    push!(label, onecold(test_labels[:, i]) - 1)\n",
    "end\n",
    "\n",
    "@show predict label\n",
    "\n",
    "mosaicview(grid; nrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2687809",
   "metadata": {},
   "source": [
    "### Upgrading the MNIST network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389e7357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 20\n",
      "error = 0.6190143302898572\n",
      "correct = 0.753\n",
      "test_error = 2.650810194516497\n",
      "test_correct = 0.7221\n",
      "=== Epoch 40\n",
      "error = 0.3953942164833337\n",
      "correct = 0.822\n",
      "test_error = 11.155884094838054\n",
      "test_correct = 0.7859\n",
      "=== Epoch 60\n",
      "error = 0.2904212420385567\n",
      "correct = 0.862\n",
      "test_error = 20.351821856372\n",
      "test_correct = 0.8177\n",
      "=== Epoch 80\n",
      "error = 0.23761920310625867\n",
      "correct = 0.874\n",
      "test_error = 28.633479336521763\n",
      "test_correct = 0.8324\n",
      "=== Epoch 100\n",
      "error = 0.2026168553027372\n",
      "correct = 0.892\n",
      "test_error = 36.10451605569889\n",
      "test_correct = 0.8422\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using MLDatasets\n",
    "using ImageCore\n",
    "using Distributions\n",
    "\n",
    "p = Binomial(1, 0.5)\n",
    "\n",
    "one_hot_encoding(x) = Flux.onehotbatch(x, 0:9)\n",
    "train_size = 1000\n",
    "\n",
    "train_x, train_y = MNIST.traindata(1:train_size);\n",
    "test_x, test_y  = MNIST.testdata();\n",
    "\n",
    "labels = one_hot_encoding(train_y)\n",
    "test_labels = one_hot_encoding(test_y)\n",
    "\n",
    "α, iterations = 2, 101\n",
    "hidden_size, pixels_per_image, num_labels = 128, 28 * 28, 10\n",
    "batch_size = 100\n",
    "\n",
    "weights₀ = 0.02 * rand(Float32, (pixels_per_image, hidden_size)) .- 0.01\n",
    "weights₁ = 0.2 * rand(Float32, (hidden_size, num_labels)) .- 0.1\n",
    "\n",
    "tanh2deriv(output) = 1 - (output ^ 2)\n",
    "\n",
    "for epoch in 1:iterations\n",
    "    error, correct = 0, 0\n",
    "    for i in 1:batch_size:train_size\n",
    "        layer₀ = reshape(train_x[:, :, i:i + batch_size - 1], (28*28, batch_size))'\n",
    "        label = labels[:, i:i + batch_size - 1]'\n",
    "\n",
    "        layer₁ = tanh.(layer₀ * weights₀)\n",
    "        dropout_mask = rand(p, size(layer₁))\n",
    "        layer₁ = (layer₁ .* dropout_mask) .* 2\n",
    "        layer₂ = softmax(layer₁ * weights₁, dims=2)\n",
    "        # @show size(layer₀) size(layer₁) size(layer₂) size(weights₀) size(weights₁)\n",
    "        error += sum(( label - layer₂ ) .^ 2)\n",
    "        correct += sum(convert.(Int, argmax(layer₂, dims=2) .== argmax(label, dims=2)))\n",
    "        layer₂∇ = (label - layer₂) ./ (batch_size * size(layer₁)[1])\n",
    "        layer₁∇ = (layer₂∇ * weights₁') .* tanh2deriv.(layer₁)\n",
    "        layer₁∇ = layer₁∇ .* dropout_mask\n",
    "        weights₁ += α * (layer₁' * layer₂∇)\n",
    "        weights₀ += α * (layer₀' * layer₁∇)\n",
    "    end\n",
    "\n",
    "    if rem(epoch, 20) == 0\n",
    "        println(\"=== Epoch $(epoch)\")\n",
    "        test_size = 10000\n",
    "        correct /= train_size\n",
    "        error /= train_size\n",
    "        @show error correct\n",
    "        test_error, test_correct = (0.0, 0)\n",
    "        for i in 1:test_size\n",
    "            layer₀ = reshape(test_x[:, :, i], 1, 28*28)\n",
    "            label = test_labels[:, i]'\n",
    "            layer₁ = tanh.(layer₀ * weights₀)\n",
    "            layer₂ = layer₁ * weights₁\n",
    "            test_error += sum(( label - layer₂ ) .^ 2)\n",
    "            test_correct += convert(Int, argmax(layer₂) == argmax(label))\n",
    "        end\n",
    "        test_error /= test_size\n",
    "        test_correct /= test_size\n",
    "        @show test_error test_correct\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac3853f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict = Any[4, 1, 2, 4, 1, 9, 6, 7, 5, 6, 4, 0, 5, 7, 1, 5]\n",
      "label = Any[4, 1, 2, 4, 1, 5, 6, 7, 5, 6, 4, 0, 3, 7, 1, 5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "28×448 MosaicView{Gray{N0f8}, 4, Base.ReshapedArray{Gray{N0f8}, 4, PaddedView{Gray{N0f8}, 3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, StackViews.StackView{Gray{N0f8}, 3, 3, Vector{OffsetArrays.OffsetMatrix{Gray{N0f8}, PaddedView{Gray{N0f8}, 2, Tuple{UnitRange{Int64}, UnitRange{Int64}}, Base.ReinterpretArray{Gray{N0f8}, 2, N0f8, Matrix{N0f8}, true}}}}}}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}}:\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                   \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux: onecold\n",
    "using Images\n",
    "\n",
    "function test_model(image)\n",
    "    layer₀ = reshape(image, 1, 28*28)\n",
    "    layer₁ = tanh.(layer₀ * weights₀)\n",
    "    layer₂ = layer₁ * weights₁\n",
    "    argmax(layer₂)[2] - 1\n",
    "end\n",
    "\n",
    "n = rand(1:10000, 16)\n",
    "grid = [\n",
    "    MNIST.convert2image(test_x[:, :, i])\n",
    "    for i in n\n",
    "]\n",
    "predict, label = [], []\n",
    "\n",
    "for i in n\n",
    "    push!(predict, test_model(test_x[:, :, i]))\n",
    "    push!(label, onecold(test_labels[:, i]) - 1)\n",
    "end\n",
    "\n",
    "@show predict label\n",
    "\n",
    "mosaicview(grid; nrow=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d646dbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDA\n",
    "CUDA.functional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36424da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA.functional() = true\n",
      "Epoch=50\n",
      "  train_loss = 0.06743283, train_accuracy = 0.992\n",
      "  test_loss = 0.30189067, test_accuracy = 0.9097\n",
      "Epoch=100\n",
      "  train_loss = 0.01513822, train_accuracy = 1.0\n",
      "  test_loss = 0.3153039, test_accuracy = 0.9134\n",
      "Epoch=150\n",
      "  train_loss = 0.0049137594, train_accuracy = 1.0\n",
      "  test_loss = 0.34140927, test_accuracy = 0.9151\n",
      "Epoch=200\n",
      "  train_loss = 0.0019735906, train_accuracy = 1.0\n",
      "  test_loss = 0.36677662, test_accuracy = 0.9155\n"
     ]
    }
   ],
   "source": [
    "using Flux, Statistics\n",
    "using Flux.Data: DataLoader\n",
    "using Flux: onehotbatch, onecold, @epochs\n",
    "using Flux.Losses: logitcrossentropy\n",
    "using Base: @kwdef\n",
    "using Statistics\n",
    "using MLDatasets\n",
    "using ImageCore\n",
    "using CUDA\n",
    "using BenchmarkTools\n",
    "\n",
    "α, iterations = 2, 201\n",
    "hidden_size, pixels_per_image, num_labels = 128, 28 * 28, 10\n",
    "batch_size = 100\n",
    "\n",
    "one_hot_encoding(x) = Flux.onehotbatch(x, 0:9)\n",
    "train_size = 2500\n",
    "\n",
    "train_x, train_y = MNIST.traindata(1:train_size);\n",
    "test_x, test_y  = MNIST.testdata();\n",
    "\n",
    "x_train, x_test = Flux.flatten(train_x), Flux.flatten(test_x)\n",
    "labels, test_labels = one_hot_encoding(train_y), one_hot_encoding(test_y)\n",
    "\n",
    "train_loader = Flux.DataLoader((x_train, labels), batchsize=batch_size, shuffle=true)\n",
    "test_loader = Flux.DataLoader((x_test, test_labels), batchsize=batch_size)\n",
    "\n",
    "\n",
    "@show CUDA.functional()\n",
    "CUDA.allowscalar(false)\n",
    "device = gpu\n",
    "\n",
    "function loss_and_accuracy(data_loader, model, device)\n",
    "    acc = 0\n",
    "    ls = 0.0f0\n",
    "    num = 0\n",
    "    for (x, y) in data_loader\n",
    "        x, y = device(x), device(y)\n",
    "        ŷ = model(x)\n",
    "        ls += logitcrossentropy(ŷ, y, agg=sum)\n",
    "        acc += sum(onecold(ŷ) .== onecold(y))\n",
    "        num +=  size(x)[end]\n",
    "    end\n",
    "    return ls / num, acc / num\n",
    "end\n",
    "\n",
    "model = Chain(\n",
    "    Dense(784, hidden_size, Flux.relu),\n",
    "    Dense(hidden_size, num_labels)\n",
    ") |> device\n",
    "\n",
    "ps = Flux.params(model)\n",
    "opt = ADAM(0.0003)\n",
    "\n",
    "\n",
    "for epoch in 1:iterations\n",
    "    for (x, y) in train_loader\n",
    "        x, y = device(x), device(y)\n",
    "        gs = gradient(() -> logitcrossentropy(model(x), y), ps)\n",
    "        Flux.Optimise.update!(opt, ps, gs)\n",
    "    end\n",
    "\n",
    "    train_loss, train_acc = loss_and_accuracy(train_loader, model, device)\n",
    "    test_loss, test_acc = loss_and_accuracy(test_loader, model, device)\n",
    "    if rem(epoch, 50) == 0\n",
    "        println(\"Epoch=$epoch\")\n",
    "        println(\"  train_loss = $train_loss, train_accuracy = $train_acc\")\n",
    "        println(\"  test_loss = $test_loss, test_accuracy = $test_acc\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c8426",
   "metadata": {},
   "source": [
    "## Neural network that understand language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12aa87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"reviews.txt\")\n",
    "raw_reviews = filter.(x -> length(x) > 0, unique.(split.(readlines(f), \" \")))\n",
    "close(f)\n",
    "\n",
    "f = open(\"labels.txt\")\n",
    "raw_labels = readlines(f)\n",
    "close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73ba7735",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = unique(reduce(vcat, raw_reviews))\n",
    "word2index = Dict(v => i for (i, v) in enumerate(vocab))\n",
    "input_dataset = map.(x -> word2index[x], raw_reviews)\n",
    "labels = map(x -> (x == \"positive\") ? 1 : 0, raw_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee731596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1\n",
      "correct / total = 0.833875\n",
      "test_correct / test_total = 0.9093333333333333\n",
      "=== Epoch 2\n",
      "correct / total = 0.8998333333333334\n",
      "test_correct / test_total = 0.9311666666666667\n",
      "=== Epoch 3\n",
      "correct / total = 0.9201666666666667\n",
      "test_correct / test_total = 0.9441666666666667\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "\n",
    "train_size = 24000\n",
    "\n",
    "α, iterations, hidden_size, batch_size = 0.01, 3, 100, 100\n",
    "\n",
    "weights₀ = 0.2 * rand(Float32, (length(vocab), hidden_size)) .- 0.1\n",
    "weights₁ = 0.2 * rand(Float32, (hidden_size, 1)) .- 0.1\n",
    "\n",
    "\n",
    "for epoch in 1:iterations\n",
    "    total, correct = 0, 0\n",
    "    for i in 1:train_size\n",
    "        x, y = input_dataset[i], labels[i]\n",
    "        \n",
    "        layer₁ = sigmoid.(sum(weights₀[x, :], dims=1))\n",
    "        layer₂ = sum(sigmoid.(layer₁ * weights₁))\n",
    "        layer₂∇ = layer₂ - y\n",
    "        layer₁∇ = layer₂∇ * weights₁'\n",
    "        weights₀[x, :] .-= layer₁∇ * α\n",
    "        weights₁ -= (layer₁' * layer₂∇) * α\n",
    "        \n",
    "        if abs(layer₂∇) < 0.5\n",
    "            correct += 1\n",
    "        end\n",
    "        total += 1\n",
    "    end\n",
    "\n",
    "    if rem(epoch, 10) != 0\n",
    "        println(\"=== Epoch $(epoch)\")\n",
    "        @show correct / total\n",
    "        test_total, test_correct = (0.0, 0)\n",
    "        for i in 1:train_size\n",
    "            x, y = input_dataset[i], labels[i]\n",
    "\n",
    "            layer₁ = sigmoid.(sum(weights₀[x, :], dims=1))\n",
    "            layer₂ = sum(sigmoid.(layer₁ * weights₁))\n",
    "            if abs(layer₂ - y) < 0.5\n",
    "                test_correct += 1\n",
    "            end\n",
    "            test_total += 1\n",
    "        end\n",
    "        @show test_correct / test_total\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0611bde",
   "metadata": {},
   "source": [
    "### Comparing word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9a401f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Vector{Pair{Any, Any}}:\n",
       "   \"beautiful\" => -0.0\n",
       "      \"lonely\" => -0.7056556329625407\n",
       " \"recommended\" => -0.7078083581416031\n",
       " \"spectacular\" => -0.7111706962582793\n",
       "       \"magic\" => -0.7126587491451116\n",
       "       \"marie\" => -0.7391234313687798\n",
       "      \"worlds\" => -0.7488965394581588\n",
       "         \"bit\" => -0.7497769035517885\n",
       "     \"freedom\" => -0.7600134527197082\n",
       "    \"flawless\" => -0.7630605198702134\n",
       "  \"underrated\" => -0.7642487332774035\n",
       "        \"best\" => -0.7654467655313637\n",
       "   \"subtitles\" => -0.7681777644865341\n",
       "       \"heart\" => -0.7722322816952291\n",
       "     \"certain\" => -0.7746179427000939"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function similar(target=\"beautiful\", len=10)\n",
    "    target_index = word2index[target]\n",
    "    scores = Dict()\n",
    "    for (word, index) in word2index\n",
    "        ∇ = weights₀[index, :] - weights₀[target_index, :]\n",
    "        ∇² = ∇ .^ 2\n",
    "        scores[word] = -sqrt(sum(∇²))\n",
    "    end\n",
    "    sort(collect(scores), by=x -> x[2], rev=true)[1:len]\n",
    "end\n",
    "\n",
    "similar(\"beautiful\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c77a9910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Vector{Pair{Any, Any}}:\n",
       "   \"excellent\" => -0.0\n",
       "     \"amazing\" => -0.783072247434673\n",
       " \"wonderfully\" => -0.815343526144588\n",
       "     \"perfect\" => -0.8190406439823616\n",
       "        \"rare\" => -0.8199391127254688\n",
       "  \"incredible\" => -0.8300515343199132\n",
       "        \"noir\" => -0.8414920201502964\n",
       "   \"wonderful\" => -0.8464807060644697\n",
       "      \"superb\" => -0.8643986063688696\n",
       "       \"loved\" => -0.8905058149461618\n",
       "  \"refreshing\" => -0.8954279358300526\n",
       "   \"enjoyable\" => -0.8965530446981935\n",
       "       \"today\" => -0.8980784862582105\n",
       "   \"perfectly\" => -0.9398194042677067\n",
       "         \"gem\" => -0.9537147370213367\n",
       "    \"funniest\" => -0.9720811391636802\n",
       " \"fascinating\" => -0.9779250115548829\n",
       "      \"highly\" => -0.9815329198755685\n",
       "    \"favorite\" => -1.0087843929478575\n",
       "       \"solid\" => -1.0254972511338174"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(\"excellent\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a9a5802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Vector{Pair{Any, Any}}:\n",
       "          \"awful\" => -0.0\n",
       "         \"poorly\" => -0.7705812653972363\n",
       "          \"waste\" => -0.8442135471653316\n",
       " \"disappointment\" => -0.9326396910457765\n",
       "          \"worst\" => -0.9834540776291226\n",
       "  \"disappointing\" => -1.002170864614918\n",
       "          \"lacks\" => -1.058728084663763\n",
       "           \"dull\" => -1.1268992238986413\n",
       "          \"fails\" => -1.187673129960059\n",
       "         \"boring\" => -1.1899847943726\n",
       "           \"mess\" => -1.195906488924898\n",
       "          \"avoid\" => -1.2515626298816966\n",
       "      \"laughable\" => -1.2813880657651135\n",
       "       \"terrible\" => -1.2819919652546716\n",
       "          \"badly\" => -1.2825153160450613\n",
       "          \"worse\" => -1.2972612616555865\n",
       "       \"annoying\" => -1.3017155674452148\n",
       "       \"horrible\" => -1.3903810117546451\n",
       "      \"redeeming\" => -1.494150315909755\n",
       "           \"poor\" => -1.4996410031702376"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(\"awful\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b99407",
   "metadata": {},
   "source": [
    "### Filling in the blank\n",
    "##### Learn richer meanings for words by having a richer signal to learn.\n",
    "\n",
    "This example uses almost exactly the same neural network as the previous one, with only a few\n",
    "modifications. First, instead of predicting a single label given a movie review, you’ll take each\n",
    "(five-word) phrase, remove one word (a focus term), and attempt to train a network to figure\n",
    "out the identity of the word you removed using the rest of the phrase. Second, you’ll use a trick\n",
    "called negative sampling to make the network train a bit faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "161ee4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b8babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"reviews.txt\")\n",
    "raw_reviews = filter.(x -> length(x) > 0, unique.(split.(readlines(f), \" \")))\n",
    "close(f)\n",
    "\n",
    "f = open(\"labels.txt\")\n",
    "raw_labels = readlines(f)\n",
    "close(f)\n",
    "\n",
    "vocab = unique(reduce(vcat, raw_reviews))\n",
    "word2index = Dict(v => i for (i, v) in enumerate(vocab))\n",
    "input_dataset = map.(x -> word2index[x], raw_reviews)\n",
    "concatenated = reduce(vcat, input_dataset)\n",
    "shuffle!(input_dataset)\n",
    "labels = map(x -> (x == \"positive\") ? 1 : 0, raw_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce3b8ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.24\n",
      "Progress: 0.48\n",
      "Progress: 0.72\n",
      "Progress: 0.96\n",
      "Progress: 0.24\n",
      "Progress: 0.48\n",
      "Progress: 0.72\n",
      "Progress: 0.96\n",
      "Progress: 0.24\n",
      "Progress: 0.48\n",
      "Progress: 0.72\n",
      "Progress: 0.96\n"
     ]
    }
   ],
   "source": [
    "using Flux: mean, sigmoid\n",
    "\n",
    "\n",
    "train_size = 24000\n",
    "\n",
    "α, iterations = 0.05, 3\n",
    "hidden_size, window, negative = 50, 2, 5\n",
    "\n",
    "weights₀ = 0.2 * (rand(Float32, (length(vocab), hidden_size)) .- 0.5)\n",
    "weights₁ = zeros(length(vocab), hidden_size)\n",
    "\n",
    "layer₂_target = zeros(negative + 1)\n",
    "layer₂_target[1] = 1\n",
    "\n",
    "for epoch in 1:iterations\n",
    "    for (i, review) in enumerate(input_dataset)\n",
    "        for (j, word_idx) in enumerate(review)\n",
    "            target_samples = vcat([word_idx], concatenated[rand(1:train_size, negative)])\n",
    "            left_context = review[max(1, j - window):j-1]\n",
    "            right_context = review[j+1:min(length(review), j + window)]\n",
    "            layer₁ = mean(weights₀[vcat(left_context, right_context), :], dims=1)\n",
    "            layer₂ = sigmoid.(layer₁ * weights₁[target_samples, :]')\n",
    "            layer₂∇ = layer₂ - layer₂_target'\n",
    "            layer₁∇ = layer₂∇ * weights₁[target_samples, :]\n",
    "            weights₀[vcat(left_context, right_context), :] .-= layer₁∇ * α\n",
    "            weights₁[target_samples, :] -= (layer₁' * layer₂∇)' * α\n",
    "        end\n",
    "        if i % 6000 == 0\n",
    "            println(\"Progress: $(i/length(input_dataset))\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c0a4f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "similar (generic function with 3 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function similar(target=\"beautiful\", len=10)\n",
    "    target_index = word2index[target]\n",
    "    scores = Dict()\n",
    "    for (word, index) in word2index\n",
    "        ∇ = weights₀[index, :] - weights₀[target_index, :]\n",
    "  ∇² = ∇ .^ 2\n",
    "        scores[word] = -sqrt(sum(∇²))\n",
    "    end\n",
    "    sort(collect(scores), by=x -> x[2], rev=true)[1:len]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b92728a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Vector{Pair{Any, Any}}:\n",
       "    \"terrible\" => -0.0\n",
       "    \"horrible\" => -3.3097364824977396\n",
       "  \"horrendous\" => -3.579971729597648\n",
       "    \"dreadful\" => -3.5812091035815334\n",
       "      \"woeful\" => -3.953273192757975\n",
       " \"nonexistent\" => -4.007442545553331\n",
       "      \"shoddy\" => -4.028205975085564\n",
       "   \"atrocious\" => -4.03973017730732\n",
       "        \"dire\" => -4.183451063085509\n",
       "      \"horrid\" => -4.185064581924523\n",
       "    \"passable\" => -4.203427356365736\n",
       "      \"patchy\" => -4.21012521558722\n",
       "     \"puerile\" => -4.217044490482364\n",
       "   \"brilliant\" => -4.229107092278999\n",
       "     \"jumbled\" => -4.258691062478422"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(\"terrible\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40463925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Vector{Pair{Any, Any}}:\n",
       "     \"beautiful\" => -0.0\n",
       " \"juxtaposition\" => -3.7550090353131163\n",
       "         \"stoic\" => -3.7661147099057706\n",
       "       \"radiant\" => -3.790207973071254\n",
       "     \"ravishing\" => -3.8204828412299014\n",
       "       \"flaming\" => -3.846823219948748\n",
       "        \"somber\" => -3.8564405127203787\n",
       "      \"lifelike\" => -3.8675046520469447\n",
       "      \"luscious\" => -3.8722490363136832\n",
       "   \"arrangement\" => -3.875301364737359\n",
       "        \"bubbly\" => -3.8871365551471513\n",
       "     \"vivacious\" => -3.8986207873937286\n",
       "          \"dark\" => -3.8990021166267685\n",
       "    \"gorgeously\" => -3.9046447460062894\n",
       "  \"technicolour\" => -3.922983824389466"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(\"beautiful\", 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f56771",
   "metadata": {},
   "source": [
    "#### Neural networks don’t really learn data; they minimize the loss function.\n",
    "\n",
    "The choice of loss function determines the\n",
    "neural network’s knowledge.\n",
    "\n",
    "The more formal term for an `error function` is a `loss function` or `objective function` (all\n",
    "three phrases are interchangeable). Considering learning to be all about minimizing a loss\n",
    "function (which includes forward propagation) gives a far broader perspective on how\n",
    "neural networks learn. Two neural networks can have identical starting weights, be trained\n",
    "over identical datasets, and ultimately learn very different patterns because you choose\n",
    "a different loss function. In the case of the two movie review neural networks, the loss\n",
    "function was different because you chose two different target values (positive or negative\n",
    "versus fill in the blank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af9460",
   "metadata": {},
   "source": [
    "### King – Man + Woman ~= Queen\n",
    "\n",
    "The task of filling in the blank creates word embeddings with interesting\n",
    "phenomena known as word analogies, wherein you can take the vectors for different words\n",
    "and perform basic algebraic operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fa76845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Pair{Any, Any}}:\n",
       "    \"terrific\" => -165.95442603500516\n",
       "    \"revolves\" => -166.2563281917494\n",
       "   \"fantastic\" => -166.30123391615814\n",
       "    \"terrible\" => -166.3048727363301\n",
       "        \"good\" => -166.38418944759124\n",
       "   \"wonderful\" => -166.51375586937112\n",
       "       \"great\" => -166.54442408638104\n",
       "        \"fine\" => -166.55153066041277\n",
       "     \"perfect\" => -166.5577899098246\n",
       " \"outstanding\" => -166.5939216572792"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function analogy(positive=[\"terrible\", \"good\"], negative=[\"bad\"], len=10)\n",
    "    norms = sum(weights₀ .^ 2, dims=2)\n",
    "    norms = reshape(norms, size(norms)[1], 1)\n",
    "    normed_weights = weights₀ .* norms\n",
    "    query_vect = zeros(length(weights₀[1, :]))\n",
    "    for word in positive\n",
    "        query_vect += normed_weights[word2index[word], :]\n",
    "    end\n",
    "    \n",
    "    for word in negative\n",
    "        query_vect -= normed_weights[word2index[word], :]\n",
    "    end\n",
    "    \n",
    "    scores = Dict()\n",
    "    for (word, index) in word2index\n",
    "        ∇ = weights₀[index, :] - query_vect\n",
    "        ∇² = ∇ .^ 2\n",
    "        scores[word] = -sqrt(sum(∇²))\n",
    "    end\n",
    "    sort(collect(scores), by=x -> x[2], rev=true)[2:len+1]\n",
    "end\n",
    "\n",
    "analogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbb2601c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Vector{Pair{Any, Any}}:\n",
       "      \"big\" => -207.89386919378092\n",
       "    \"large\" => -208.53089064977726\n",
       "    \"major\" => -209.20847656174945\n",
       "      \"par\" => -209.51079797398125\n",
       "      \"low\" => -209.5921421215758\n",
       "   \"bottom\" => -209.60442302532417\n",
       "  \"biggest\" => -209.69713979470885\n",
       "  \"massive\" => -209.7059421052052\n",
       " \"enormous\" => -209.78333074585044\n",
       "   \"rental\" => -209.82269079979795\n",
       "      \"box\" => -209.86474032331722\n",
       "   \"proper\" => -209.98485789573044\n",
       " \"greatest\" => -210.00528997751357\n",
       "     \"wide\" => -210.0355564517427\n",
       "    \"giant\" => -210.05712404592452\n",
       "     \"high\" => -210.06939895366375\n",
       "     \"soft\" => -210.07419025745145\n",
       "   \"single\" => -210.09380694130283\n",
       "   \"filled\" => -210.10660714868362\n",
       "   \"decent\" => -210.12894755462403"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy([\"big\", \"large\"], [\"small\"], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50e0b01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Vector{Pair{Any, Any}}:\n",
       "         \"awful\" => -221.41499143769627\n",
       "     \"appalling\" => -221.8751880988994\n",
       "           \"bad\" => -221.92404526258653\n",
       "     \"atrocious\" => -222.17358387971976\n",
       "  \"unbelievable\" => -222.17401541410243\n",
       "       \"unfunny\" => -222.27795596528725\n",
       "        \"wooden\" => -222.33253119712518\n",
       "    \"incoherent\" => -222.34435245745195\n",
       "       \"excited\" => -222.36231331998277\n",
       "    \"amateurish\" => -222.4930040687682\n",
       "      \"terrible\" => -222.55067136333196\n",
       "          \"lame\" => -222.58926220589964\n",
       "    \"uninspired\" => -222.61324757760195\n",
       "       \"abysmal\" => -222.63025959067124\n",
       " \"uninteresting\" => -222.63933941389587\n",
       "    \"incredible\" => -222.64511527375166\n",
       "  \"embarrassing\" => -222.68251023400236\n",
       "   \"predictable\" => -222.6847414175989\n",
       "         \"forth\" => -222.68593727976275\n",
       "      \"dreadful\" => -222.72407417527504"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy([\"awesome\", \"bad\"], [\"good\"], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a7a76",
   "metadata": {},
   "source": [
    "## 12. Neural network that write like Shakespeare\n",
    "Recurrent layers for variable-length data\n",
    "\n",
    "### The challenge of arbitrary length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "85e709cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "norms = sum(weights₀ .^ 2, dims=2)\n",
    "norms = reshape(norms, size(norms)[1], 1)\n",
    "normed_weights = weights₀ .* norms\n",
    "\n",
    "function make_sent_vect(words)\n",
    "    indices = map(x -> word2index[x], words)\n",
    "    vec(mean(normed_weights[indices, :], dims=1))\n",
    "end\n",
    "\n",
    "reviews2vectors = map(x -> make_sent_vect(x), raw_reviews);\n",
    "reviews2vectors = permutedims(hcat(reviews2vectors...));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4bd19c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250000,)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews2vectors = reduce(vcat, reviews2vectors);\n",
    "\n",
    "size(reviews2vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "69b23fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "when i got this movie free from my job along with three other similar movies . watched then very low expectations now isn t bad per se you get what pay for it is a tale of love betrayal lies sex scandal everything want in definitely not hollywood blockbuster but\n",
      "======\n",
      "how has this piece of crap stayed on tv long it s terrible . makes me want to shoot someone so fake that is actually worse than a sci fi movie i d rather have stroke watch nonsense remember watching when first came out thought hey could be interesting then\n",
      "======\n",
      "this show comes up with interesting locations as fast the travel channel . it is billed reality but in actuality pure prime time soap opera s tries to use exotic locales a facade bring people into phony contest then proceeds hook viewers on contestants style br also borrows from an\n"
     ]
    }
   ],
   "source": [
    "using StatsBase: countmap\n",
    "\n",
    "function reverselookup(d, v)\n",
    "    for k in keys(d)\n",
    "        if d[k] == v\n",
    "            return k\n",
    "        end\n",
    "    end\n",
    "    error(\"LookupError\")\n",
    "end\n",
    "\n",
    "function most_similar_reviews(review)\n",
    "    v = make_sent_vect(review)\n",
    "    scores = Dict(i => val for (i, val) in enumerate(reviews2vectors * v))\n",
    "    \n",
    "    vals = values(scores)\n",
    "    c = countmap(vals)\n",
    "\n",
    "    most_similar = []\n",
    "    for (val, _) in sort(collect(c), by=x -> x[2], rev=true)[1:3]\n",
    "        idx = reverselookup(scores, val)\n",
    "        push!(most_similar, raw_reviews[idx])\n",
    "    end\n",
    "    most_similar\n",
    "end\n",
    "\n",
    "for res in map(x -> join(x[1:50], \" \"), most_similar_reviews([\"ok\", \"terrible\"]))\n",
    "    println(\"======\")\n",
    "    println(res)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ddea2",
   "metadata": {},
   "source": [
    "### Using identity vectors to sum word embeddings\n",
    "\n",
    "You may think identity matrices are useless. What’s the purpose of a matrix that takes a\n",
    "vector and outputs that same vector? In this case, we’ll use it as a teaching tool to show how\n",
    "to set up a more complicated way of summing the word embeddings so the neural network\n",
    "can take order into account when generating the final sentence embedding. Let’s explore\n",
    "another way of summing embeddings.\n",
    "\n",
    "\n",
    "### Matrices that change absolutely nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "782adda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1ad67d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = [1 0 0; 0 1 0; 0 0 1]\n",
      "a * i = [1 2 3]\n",
      "b * i = [0.1 0.2 0.3]\n",
      "c * i = [-1.0 -0.5 0.0]\n",
      "d * i = [0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×3 Matrix{Int64}:\n",
       " 0  0  0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1 2 3]\n",
    "b = [0.1 0.2 0.3]\n",
    "c = [-1 -0.5 0]\n",
    "d = [0 0 0]\n",
    "\n",
    "i = Matrix(1I, 3, 3)\n",
    "\n",
    "@show i a * i b * i c * i d * i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d9ecb510",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this + movie + rocks = [13 15 17]\n",
      "(this * i + movie) * i + rocks = [13 15 17]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×3 Matrix{Int64}:\n",
       " 13  15  17"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this = [2 4 6]\n",
    "movie = [10 10 10]\n",
    "rocks = [1 1 1]\n",
    "\n",
    "@show this + movie + rocks\n",
    "@show ((this * i) + movie) * i + rocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4ff3a",
   "metadata": {},
   "source": [
    "#### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "72138574",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×9 Matrix{Float64}:\n",
       " 0.111111  0.111111  0.111111  0.111111  …  0.111111  0.111111  0.111111"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux: softmax\n",
    "\n",
    "words = [\"yankees\", \"bears\", \"braves\", \"red\", \"sox\", \"lose\", \"defeat\", \"beat\", \"tie\"]\n",
    "word_vects = Dict(i => [0.0 0.0 0.0] for i in words)\n",
    "sent2output = rand(3, length(word_vects))\n",
    "identity_matrix = Matrix(1.0I, 3, 3)\n",
    "\n",
    "layer₀ = word_vects[\"red\"]\n",
    "layer₁ = layer₀ * identity_matrix + word_vects[\"sox\"]\n",
    "layer₂ = layer₁ * identity_matrix + word_vects[\"defeat\"]\n",
    "\n",
    "pred = softmax(layer₂ * sent2output, dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "95ac77a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×9 Matrix{Float64}:\n",
       " 0.299183  0.168145  0.852319  0.154044  …  0.182136  0.244148  0.0725293\n",
       " 0.897864  0.522778  0.820588  0.377823     0.464942  0.689668  0.925029\n",
       " 0.817366  0.181134  0.567171  0.837345     0.527068  0.276205  0.144064"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1 0 0 0 0 0 0 0 0]\n",
    "\n",
    "∇ = pred - y\n",
    "layer₂∇ = ∇ * sent2output'\n",
    "defeat∇ = layer₂∇ * 1\n",
    "layer₁∇ = layer₂∇ * identity_matrix'\n",
    "sox∇ = layer₁∇ * 1\n",
    "layer₀∇ = layer₁∇ * identity_matrix'\n",
    "α = 0.01\n",
    "\n",
    "word_vects[\"red\"] -= layer₀∇ * α\n",
    "word_vects[\"sox\"] -= sox∇ * α\n",
    "word_vects[\"defeat\"] -= defeat∇ * α\n",
    "\n",
    "identity_matrix .-= layer₀ .* layer₁∇ * α\n",
    "identity_matrix .-= layer₁ .* layer₂∇ * α\n",
    "sent2output -= layer₂' * ∇ * α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ed25ea08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Matrix{Float64}} with 9 entries:\n",
       "  \"defeat\"  => [-0.0100027 0.0294717 0.0548109]\n",
       "  \"tie\"     => [0.0 0.0 0.0]\n",
       "  \"lose\"    => [0.0 0.0 0.0]\n",
       "  \"beat\"    => [0.0 0.0 0.0]\n",
       "  \"yankees\" => [0.0 0.0 0.0]\n",
       "  \"sox\"     => [-0.0100027 0.0294717 0.0548109]\n",
       "  \"red\"     => [-0.0100027 0.0294717 0.0548109]\n",
       "  \"bears\"   => [0.0 0.0 0.0]\n",
       "  \"braves\"  => [0.0 0.0 0.0]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e043af",
   "metadata": {},
   "source": [
    "#### You have all the tools; let’s train the network on a toy corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "c989e4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bathroom\""
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function clean(word)\n",
    "    word = strip(word)\n",
    "    word = replace(word, r\"\\s+[\\d]+\" => \"\", \"\\n\" => \"\", r\"[.?]\" => \"\")\n",
    "end\n",
    "\n",
    "clean(\"\\tbathroom\\t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "bde1e049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(tokens) = (1000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10×1 Matrix{Vector{String}}:\n",
       " [\"Mary\", \"moved\", \"to\", \"the\", \"bathroom\"]\n",
       " [\"John\", \"went\", \"to\", \"the\", \"hallway\"]\n",
       " [\"Where\", \"is\", \"Mary\", \"bathroom\"]\n",
       " [\"Daniel\", \"went\", \"back\", \"to\", \"the\", \"hallway\"]\n",
       " [\"Sandra\", \"moved\", \"to\", \"the\", \"garden\"]\n",
       " [\"Where\", \"is\", \"Daniel\", \"hallway\"]\n",
       " [\"John\", \"moved\", \"to\", \"the\", \"office\"]\n",
       " [\"Sandra\", \"journeyed\", \"to\", \"the\", \"bathroom\"]\n",
       " [\"Where\", \"is\", \"Daniel\", \"hallway\"]\n",
       " [\"Mary\", \"moved\", \"to\", \"the\", \"hallway\"]"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = map.(\n",
    "    clean, \n",
    "    map(x -> x[2:end], split.(readlines(\"./static/data/tasksv11/en/qa1_single-supporting-fact_train.txt\")[1:1000], \" \"))\n",
    ")\n",
    "@show size(tokens)\n",
    "tokens[1:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "a06ba561",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = unique(reduce(vcat, tokens))\n",
    "word2index = Dict(word => i for (i, word) in enumerate(vocab))\n",
    "words2indices(sentence) = map(x -> word2index[x], sentence);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "89d56ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = length(vocab)\n",
    "embed_size = 10\n",
    "embed = (rand(vocab_len, embed_size) .- 0.5) * 0.1\n",
    "recurrent = Matrix(1I, embed_size, embed_size)\n",
    "start = zeros((1, embed_size))\n",
    "decoder = (rand(embed_size, vocab_len) .- 0.5) * 0.1\n",
    "one_hot = Matrix(1I, vocab_len, vocab_len);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "e1a89091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Dict(\"hidden\" => [0.0 0.0 … 0.0 0.0]), Dict(\"hidden\" => [0.04243500900075005 -0.03157065298572253 … -0.019127070657978775 -0.013729220660951247], \"pred\" => [0.05263157894736842 0.05263157894736842 … 0.05263157894736842 0.05263157894736842]), Dict(\"hidden\" => [0.05947617670571758 -0.03134713697622341 … -0.02216625033006814 -0.006462289726639314], \"pred\" => [0.05255912581454881 0.05283835962806805 … 0.052646270054151484 0.052358356514606255]), Dict(\"hidden\" => [0.026244755723235647 0.0031055225310064466 … -0.07124711699858233 -0.0073420897536108365], \"pred\" => [0.052529845001404424 0.0528472766184669 … 0.05266605177149329 0.05232904326636195]), Dict(\"hidden\" => [-0.009142009740404411 -0.03786494678850315 … -0.08701985069337688 -0.03932719216569696], \"pred\" => [0.05237165302531646 0.05292822675346737 … 0.05254187947470339 0.05220309747155011]), Dict(\"hidden\" => [0.014656925650682495 -0.047511993109032646 … -0.05522644934889257 -0.00023345162548763648], \"pred\" => [0.052291101312322216 0.05299012795071704 … 0.05247653816550962 0.05206546250067414])], 14.723386732746281)"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(sent)\n",
    "    layer = Dict(\"hidden\" => start)\n",
    "    layers = [layer]\n",
    "    \n",
    "    loss = 0\n",
    "    preds = []\n",
    "    for target_i in 1:length(sent)\n",
    "        layer = Dict(\"pred\" => softmax(last(layers)[\"hidden\"] * decoder, dims=2))\n",
    "        loss += -log(layer[\"pred\"][sent[target_i]])\n",
    "        layer[\"hidden\"] = reshape((last(layers)[\"hidden\"] * recurrent)' + embed[sent[target_i], :], (1, 10))\n",
    "        push!(layers, layer)\n",
    "    end\n",
    "    layers, loss\n",
    "end\n",
    "predict(map(x -> word2index[x], [\"Mary\", \"moved\", \"to\", \"the\", \"bathroom\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b64fe",
   "metadata": {},
   "source": [
    "### Backpropagation with arbitrary length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "441fe65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 19.011440750297293\n",
      "Perplexity: 18.974382770679195\n",
      "Perplexity: 18.925668801474448\n",
      "Perplexity: 18.852162885853076\n",
      "Perplexity: 18.732288533032015\n",
      "Perplexity: 18.525510253505157\n",
      "Perplexity: 18.145149064934145\n",
      "Perplexity: 17.375841989491132\n",
      "Perplexity: 15.679406620301663\n",
      "Perplexity: 13.69908693617188\n",
      "Perplexity: 12.649445245135222\n",
      "Perplexity: 11.76793254865784\n",
      "Perplexity: 11.120110966694744\n",
      "Perplexity: 10.72879852993606\n",
      "Perplexity: 10.357723529281486\n",
      "Perplexity: 9.810597381537782\n",
      "Perplexity: 8.979607588807095\n",
      "Perplexity: 7.928116394713861\n",
      "Perplexity: 6.99502115430651\n",
      "Perplexity: 6.397554985926519\n",
      "Perplexity: 5.881598260895798\n",
      "Perplexity: 5.429404231961539\n",
      "Perplexity: 5.125858599850172\n",
      "Perplexity: 4.946045912985343\n",
      "Perplexity: 4.836281531429737\n",
      "Perplexity: 4.7576243579310535\n",
      "Perplexity: 4.693233311204678\n",
      "Perplexity: 4.6410488429398695\n",
      "Perplexity: 4.599800555252503\n",
      "Perplexity: 4.566025416563938\n",
      "Perplexity: 4.5360317888135695\n",
      "Perplexity: 4.506437077535431\n",
      "Perplexity: 4.4742650892895215\n",
      "Perplexity: 4.437270436390585\n",
      "Perplexity: 4.394242555685701\n",
      "Perplexity: 4.344977752882222\n",
      "Perplexity: 4.289827888973024\n",
      "Perplexity: 4.229189527876928\n",
      "Perplexity: 4.163364222388349\n",
      "Perplexity: 4.092647099784515\n"
     ]
    }
   ],
   "source": [
    "for epoch in 1:40\n",
    "    for i in 1:length(tokens)  \n",
    "        α = 0.001\n",
    "        sent = words2indices(tokens[i][2:end])\n",
    "        len = length(sent)\n",
    "        layers, loss = predict(sent)\n",
    "        \n",
    "        for layer_idx in length(layers):-1:1\n",
    "            layer = layers[layer_idx]\n",
    "            if layer_idx > 1\n",
    "                target = sent[layer_idx - 1]\n",
    "                layer[\"output∇\"] = layer[\"pred\"] - one_hot[target, :]'\n",
    "                \n",
    "                new_hidden∇ = layer[\"output∇\"] * decoder'\n",
    "                if layer_idx == length(layers)\n",
    "                    layer[\"hidden∇\"] = new_hidden∇\n",
    "                else\n",
    "                    layer[\"hidden∇\"] = new_hidden∇ + layers[layer_idx + 1][\"hidden∇\"] * recurrent'\n",
    "                end\n",
    "            else\n",
    "                target = last(sent)\n",
    "                layer[\"hidden∇\"] = layers[layer_idx + 1][\"hidden∇\"] * recurrent'\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        start -= layers[1][\"hidden∇\"] * α / len\n",
    "        \n",
    "        for (layer_idx, layer) in enumerate(layers[2:end])\n",
    "            decoder -= layers[layer_idx][\"hidden\"]' .* layer[\"output∇\"] * α / len\n",
    "            embed_idx = sent[layer_idx]\n",
    "            embed[embed_idx, :] -= layers[layer_idx][\"hidden∇\"]' * α / len\n",
    "            \n",
    "            recurrent -= layers[layer_idx][\"hidden\"]' .* layer[\"hidden∇\"] * α / len\n",
    "        end\n",
    "        \n",
    "        if rem(i, 1000) == 0\n",
    "            println(\"Perplexity: $(exp(loss/len))\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "3f422124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Daniel\", \"travelled\", \"to\", \"the\", \"bedroom\"]\n",
      "Prev Input: Daniel\tTrue: travelled\tPred: is\n",
      "Prev Input: travelled\tTrue: to\tPred: to\n",
      "Prev Input: to\tTrue: the\tPred: the\n",
      "Prev Input: the\tTrue: bedroom\tPred: bedroom\n"
     ]
    }
   ],
   "source": [
    "sent_index = 233\n",
    "\n",
    "l, _ = predict(words2indices(tokens[sent_index]))\n",
    "println(tokens[sent_index])\n",
    "\n",
    "for (i, each_layer) in enumerate(l[2:end-1])\n",
    "    input = tokens[sent_index][i]\n",
    "    correct = tokens[sent_index][i + 1]\n",
    "    pred = vocab[argmax(vec(each_layer[\"pred\"]))]\n",
    "    println(\"Prev Input: $input\\tTrue: $correct\\tPred: $pred\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ece18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6324151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15037529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fdc40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab7e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0260b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b9e5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1283649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2ba6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfacbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56694a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061593d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc384bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe8361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
